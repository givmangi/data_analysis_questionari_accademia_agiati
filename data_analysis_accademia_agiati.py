"""
Analisi Quantitativa dei Dati di Soddisfazione - Accademia Roveretana degli Agiati
================================================================================

Autore: Giuseppe Pio Mangiacotti
Istituzione: Universit√† degli Studi di Trento - Dipartimento di Scienze Cognitive
Anno Accademico: 2024/2025
Versione: 2.1 (CORREZIONE METODOLOGICA CRITICA - Fisher Exact Test) + GRAFICI TESI

‚ö†Ô∏è  AGGIORNAMENTO CRITICO: 
Questa versione corregge un errore metodologico significativo nella v2.0.
Il test Mann-Whitney U √® stato sostituito con Fisher Exact Test per 
dati essenzialmente binari (solo valori 2 e 3).

üéì AGGIORNAMENTO TESI:
Aggiunti grafici specifici per l'elaborato finale:
- Pannello demografico completo (et√†, soci, scolarizzazione)
- Pannello modalit√† partecipazione (cartaceo/webinar, argomenti)

Descrizione:
Script per l'analisi completa del dataset di feedback raccolto presso l'Accademia 
Roveretana degli Agiati. Implementa algoritmi statistici appropriati per 
l'identificazione dei key findings principali e la generazione del Cultural 
Engagement Score (CES).

Correzioni Metodologiche v2.1:
===============================
üéØ PROBLEMA RISOLTO:
- Test Mann-Whitney U inappropriato per dati con ties eccessivi (100% ties)
- Solo valori 2 (Soddisfatto) e 3 (Molto Soddisfatto) nel dataset
- Perdita significativa di potenza statistica

‚úÖ SOLUZIONE IMPLEMENTATA:
- Fisher Exact Test per confronto proporzioni binarie
- Tabelle di contingenza 2√ó2 appropriate
- Odds ratio e p-value esatti
- Warning automatici per test inappropriati
- Validazione qualit√† statistica integrata
- REGRESSIONE LOGISTICA per validazione terziaria completa

Metodologia Statistica (Corretta):
===================================
PRIMARI:
- Fisher Exact Test per confronti binari/quasi-binari ‚úÖ
- Test œá¬≤ per tabelle contingenza (con correzioni appropriate) ‚úÖ
- Bootstrap resampling per validazione indicatori compositi ‚úÖ
- Regressione logistica per validazione crociata ‚úÖ

SECONDARI (con disclaimer):
- Mann-Whitney U mantenuto per compatibilit√† ‚ö†Ô∏è
- Coefficiente r di Rosenthal per effect size ‚ö†Ô∏è

Dipendenze:
- pandas >= 1.5.0
- matplotlib >= 3.6.0  
- seaborn >= 0.12.0
- numpy >= 1.24.0
- scipy >= 1.10.0 (fisher_exact, chi2_contingency, mannwhitneyu)
- scikit-learn >= 1.0.0 (LogisticRegression - opzionale)
- plotly >= 5.15.0 (opzionale)

Utilizzo:
python data_analysis_accademia_agiati_v2.1.py --input paper_report.CSV --output ./output/

Citazione (Aggiornata):
=======================
Mangiacotti, G.P. (2025). Progettazione di interfacce per la raccolta di feedback 
nel settore culturale: un sistema ibrido per l'Accademia Roveretana degli Agiati v2.1
Universit√† degli Studi di Trento.
"""



"""
Utilizzo:
python data_analysis_accademia_agiati.py --input paper_report.CSV --output ./output/
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import chi2_contingency, pearsonr, mannwhitneyu, ks_2samp, rankdata, fisher_exact, spearmanr, norm
import warnings
from sklearn.linear_model import LogisticRegression

import argparse
import os
from collections import Counter
from datetime import datetime
from pathlib import Path
import json

warnings.filterwarnings('ignore')
PURPLE_PALETTE = {
    'primary_purple': '#6A4C93',
    'light_purple': '#A47FC7', 
    'dark_purple': '#4A2C70',  
    'lavender': '#C8B8E8',           
    'royal_purple': '#8B5A9F', 
    'plum': '#9B59B6',                
    'periwinkle': '#8E7CC3',     
    'sage': '#87A96B',          
    'slate_blue': '#6C7B95',   
    'warm_gold': '#DAA520',    
    'soft_teal': '#5D8A8A',    
    'dusty_rose': '#C49BB0'           
}
PURPLE_SEQUENTIAL = ['#F3F0FF', '#E6DBFF', '#D9C5FF', '#CCB0FF', '#BF9BFF', '#B386FF', '#A670FF', '#995BFF', '#8C46FF', '#7F31FF']
PURPLE_CATEGORICAL = ['#6A4C93', '#87A96B', '#8E7CC3', '#C49BB0', '#5D8A8A', '#DAA520', '#9B59B6', '#6C7B95']

plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette(PURPLE_CATEGORICAL)

class AccademiaAnalyzer:
    """
    Classe principale per l'analisi dei dati dell'Accademia degli Agiati.
    ====================================================================================
    Implementa metodologie quantitative appropriate per l'identificazione di pattern
    di soddisfazione, engagement e comportamento del pubblico culturale.
    OUTPUT GENERATI:
    ---------------
    - analysis_report_corrected.txt: Report con metodologia corretta
    - methodological_warning.txt: Avviso sui problemi metodologici
    - contingency_table_fisher_test.csv: Tabella per Fisher test
    - digital_satisfaction_paradox_corrected.png: Visualizzazione aggiornata
    - demographic_overview_panel.png: üéì Pannello demografico (TESI)
    - participation_modalities_panel.png: üéì Pannello partecipazione (TESI)
    """
    def __init__(self, csv_path: str, output_dir: str = "./output/"):
        """
        Inizializza l'analyzer con metodologia corretta.
        Parametri:
        -----------
        csv_path : str
            Percorso al file CSV dei dati
        output_dir : str
            Directory di output (default: ./output/)
        """
        self.csv_path = csv_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        plt.rcParams.update({
            'figure.figsize': (12, 8),
            'font.size': 11,
            'axes.titlesize': 14,
            'axes.labelsize': 12,
            'xtick.labelsize': 10,
            'ytick.labelsize': 10,
            'legend.fontsize': 10,
            'figure.dpi': 300,
            'savefig.dpi': 300,
            'savefig.bbox': 'tight',
            'axes.prop_cycle': plt.cycler('color', PURPLE_CATEGORICAL)
        })
        self.results = {}
        self.methodological_warnings = []
        self.test_appropriateness = {}
        
    def _log_methodological_warning(self, test_name: str, warning: str):
        """
        Log avvisi metodologici per revisione.

        params:
        -----------
        test_name : str
            Nome del test statistico
        warning : str
            Descrizione del problema metodologico
        """
        self.methodological_warnings.append({
            'test': test_name,
            'warning': warning,
            'timestamp': datetime.now().isoformat()
        })
        print(f"AVVISO METODOLOGICO [{test_name}]: {warning}")
    
    def _assess_test_appropriateness(self, test_name: str, data_description: str, is_appropriate: bool, reason: str):
        """
        Valuta e registra l'appropriatezza di un test statistico.

        params:
        -----------
        test_name : str
            Nome del test statistico
        data_description : str
            Descrizione dei dati analizzati
        is_appropriate : bool
            Se il test √® appropriato
        reason : str
            Motivazione della valutazione
        """
        self.test_appropriateness[test_name] = {
            'data_description': data_description,
            'is_appropriate': is_appropriate,
            'reason': reason,
            'recommendation': 'Utilizzare' if is_appropriate else 'Evitare'
        }
        if not is_appropriate:
            self._log_methodological_warning(test_name, reason)
        
    def load_and_clean_data(self) -> pd.DataFrame:
        """
        Carica e pulisce il dataset applicando standardizzazioni e validazioni.
        returns:
        --------
        pd.DataFrame
            Dataset pulito e standardizzato
        """
        print("Caricamento e pulizia del dataset...")
        try:
            df = pd.read_csv(self.csv_path, delimiter=';', encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(self.csv_path, delimiter=';', encoding='cp1252')
        df.columns = df.columns.str.strip()
        df = self._standardize_age_categories(df)
        df = self._standardize_membership_status(df)
        df = self._standardize_satisfaction_scores(df)
        df = self._clean_communication_channels(df)
        # filtro esclusione record incompleti per colonne essenziali
        required_cols = ['eta_std', 'socio_std', 'soddisfazione_num']
        df_clean = df.dropna(subset=required_cols)
        # log statistiche pulizia
        print(f"\tDataset originale: {len(df)} record")
        print(f"\tDataset pulito: {len(df_clean)} record")
        print(f"\tTasso completezza: {len(df_clean)/len(df)*100:.1f}%")
        self.df = df_clean
        return df_clean
    
    def _standardize_age_categories(self, df: pd.DataFrame) -> pd.DataFrame:
        """Standardizza le categorie di et√†."""
        #solo un double-check
        age_mapping = {
            '14-30': '14-30',
            '31-50': '31-50', 
            '51-70': '51-70',
            '>70': '>70',
            '70>': '>70',
            '14-30 ': '14-30'  
        }
        df['eta_std'] = df['Et√†'].str.strip().map(age_mapping)
        return df
    
    def _standardize_membership_status(self, df: pd.DataFrame) -> pd.DataFrame:
        """Standardizza lo status di socio."""
        #ennesimo double-check
        membership_mapping = {
            'S√¨': 'S√¨',
            'S√≠': 'S√¨', 
            'Si': 'S√¨',
            'No': 'No'
        }
        df['socio_std'] = df['Socio'].map(membership_mapping)
        return df
    
    def _standardize_satisfaction_scores(self, df: pd.DataFrame) -> pd.DataFrame:
        """Converte e valida i punteggi di soddisfazione."""
        df['soddisfazione_num'] = pd.to_numeric(
            df['Soddisfazione (1-3)'], errors='coerce'
        )
        valid_range = df['soddisfazione_num'].between(1, 3, inclusive='both')
        df.loc[~valid_range, 'soddisfazione_num'] = np.nan
        return df
    
    def _clean_communication_channels(self, df: pd.DataFrame) -> pd.DataFrame:
        """Pulisce e standardizza i canali di comunicazione."""
        if 'Conoscenza' in df.columns:
            df['canali_clean'] = df['Conoscenza'].fillna('N/A')
        return df
    
    def calculate_key_findings(self) -> dict:
        """
        Calcola i tre key findings principali identificati nell'analisi.
        
        Returns:
        --------
        dict
            Dizionario con tutti i key findings e relative statistiche
        """
        print("Calcolo Key Findings...")        
        findings = {}
        findings['digital_paradox'] = self._analyze_digital_satisfaction_paradox()
        findings['generational_gap'] = self._analyze_generational_membership_gap()
        findings['contentment_effect'] = self._analyze_inverse_contentment_effect()
        self.results['key_findings'] = findings
        return findings
    
    def _analyze_digital_satisfaction_paradox(self) -> dict:
        """
        Analizza il Digital Satisfaction Paradox usando metodologie statistiche appropriate.
        CORREZIONE METODOLOGICA v2.1:
        - Fisher Exact Test per confronto proporzioni se dati essenzialmente binari
        - Mann-Whitney U mantenuto come analisi secondaria con limitazioni
        - Aggiunta regressione logistica per robustezza, seppur omessa nella tesi per semplicit√†
        """
        webinar_data = self.df[self.df['Fonte'] == 'Webinar']['soddisfazione_num']
        cartaceo_data = self.df[self.df['Fonte'] == 'Cartaceo']['soddisfazione_num']
        stats_webinar = {
            'n': len(webinar_data),
            'mean': webinar_data.mean(),
            'std': webinar_data.std(),
            'median': webinar_data.median(),
            'satisfaction_rate_max': (webinar_data == 3).mean(),  # % "Molto Soddisfatti"
            'satisfaction_rate_med': (webinar_data == 2).mean()   # % "Soddisfatti"
        }
        stats_cartaceo = {
            'n': len(cartaceo_data),
            'mean': cartaceo_data.mean(), 
            'std': cartaceo_data.std(),
            'median': cartaceo_data.median(),
            'satisfaction_rate_max': (cartaceo_data == 3).mean(),
            'satisfaction_rate_med': (cartaceo_data == 2).mean()
        }
        # === TEST ESATTO DI FISHER (METODO PRINCIPALE) ===
        contingency_table = [
            [int((cartaceo_data == 2).sum()), int((cartaceo_data == 3).sum())],  # cartaceo: [sodd, m. sodd]
            [int((webinar_data == 2).sum()), int((webinar_data == 3).sum())]     # webinar: [sodd, m. sodd]
        ]
        # Fisher Exact Test
        odds_ratio, fisher_p_value = stats.fisher_exact(contingency_table)
        # calcolo manuale delle proporzioni per verifica
        prop_webinar_very_sat = stats_webinar['satisfaction_rate_max']
        prop_cartaceo_very_sat = stats_cartaceo['satisfaction_rate_max']
        proportion_difference = prop_webinar_very_sat - prop_cartaceo_very_sat
        # assessment appropriatezza Fisher test
        min_expected_frequency = min([min(row) for row in contingency_table])
        self._assess_test_appropriateness(
            'Fisher Exact Test',
            'Confronto proporzioni tra modalit√† cartacea e webinar',
            True,
            'Appropriato per tabelle 2√ó2, nessuna assunzione violata' if min_expected_frequency >= 5 else 'Inappropriato: frequenze attese troppo basse'
        )
        # === MANN-WHITNEY U (ANALISI SECONDARIA) ===
        u_statistic, p_value_mw = stats.mannwhitneyu(
            webinar_data, cartaceo_data, 
            alternative='two-sided',
            use_continuity=True
        )
        n1, n2 = len(webinar_data), len(cartaceo_data)
        total_n = n1 + n2
        # z-score 
        mean_u = n1 * n2 / 2
        std_u = np.sqrt(n1 * n2 * (total_n + 1) / 12)
        z_score = (u_statistic - mean_u) / std_u
        rosenthal_r = abs(z_score) / np.sqrt(total_n)
        combined_data = np.concatenate([cartaceo_data, webinar_data])
        group_labels = np.concatenate([
            np.zeros(len(cartaceo_data)),  # 0 = cartaceo
            np.ones(len(webinar_data))     # 1 = webinar
        ])
        ranks = stats.rankdata(combined_data)
        cartaceo_rank_mean = ranks[group_labels == 0].mean()
        webinar_rank_mean = ranks[group_labels == 1].mean()
        # assessment problemi Mann-Whitney
        total_values = len(np.unique(combined_data))
        ties_percentage = 1 - (total_values / total_n)
        mw_problematic = ties_percentage > 0.5  # >50% ties considerato problematico per letteratura
        self._assess_test_appropriateness(
            'Mann-Whitney U',
            f'Dati ordinali con {ties_percentage:.1%} ties',
            not mw_problematic,
            f'Ties eccessivi ({ties_percentage:.1%}), solo {total_values} valori distinti' if mw_problematic else 'Appropriato'
        )
        
        # === REGRESSIONE LOGISTICA (ANALISI TERZIARIA OMESSA) ===
        logistic_results = self._analyze_logistic_regression_validation(webinar_data, cartaceo_data)
        # === ANALISI ET√† ===
        age_mapping = {'14-30': 22, '31-50': 40, '51-70': 60, '>70': 75}
        age_webinar = self.df[self.df['Fonte'] == 'Webinar']['eta_std'].map(age_mapping).mean()
        age_cartaceo = self.df[self.df['Fonte'] == 'Cartaceo']['eta_std'].map(age_mapping).mean()
        return {
            # === RISULTATI PRINCIPALI (FISHER TEST) ===
            'primary_test': 'Fisher Exact Test',
            'fisher_odds_ratio': odds_ratio,
            'fisher_p_value': fisher_p_value,
            'contingency_table': contingency_table,
            'proportion_difference': proportion_difference,
            # === STATISTICHE DESCRITTIVE ===
            'webinar_stats': stats_webinar,
            'cartaceo_stats': stats_cartaceo,
            'difference': stats_webinar['mean'] - stats_cartaceo['mean'],
            # === MANN-WHITNEY (SECONDARIO CON DISCLAIMER) ===
            'mannwhitney_u': u_statistic,
            'mannwhitney_p': p_value_mw,
            'rosenthal_r': rosenthal_r,
            'rosenthal_r_squared': rosenthal_r**2,
            'z_score': z_score,
            'cartaceo_rank_mean': cartaceo_rank_mean,
            'webinar_rank_mean': webinar_rank_mean,
            'mw_methodological_warning': mw_problematic,
            'ties_percentage': ties_percentage,
            # === REGRESSIONE LOGISTICA COMPLETA ===
            'logistic_regression': logistic_results,
            # === ANALISI ET√Ä ===
            'age_webinar': age_webinar,
            'age_cartaceo': age_cartaceo,
            'age_paradox': age_webinar - age_cartaceo,
            # === LEGACY COMPATIBILITY ===
            'cohens_d': rosenthal_r * 2,  # Approssimazione per compatibilit√† grafici
            't_statistic': z_score,       # Compatibilit√†
            'p_value': fisher_p_value,    # USA P-VALUE FISHER come principale
            'ks_statistic': np.nan,       # Non applicabile per dati binari
            'ks_p_value': np.nan
        }
    
    def _analyze_logistic_regression_validation(self, webinar_data, cartaceo_data):
        """
        Implementa regressione logistica per validazione del Digital Satisfaction Paradox.
        
        Returns:
        --------
        dict
            Risultati completi della regressione logistica con statistiche inferenziali
        """
        try:
            X = np.array([1 if fonte == 'Webinar' else 0 for fonte in self.df['Fonte']]).reshape(-1, 1)
            y = np.array([1 if sat == 3 else 0 for sat in self.df['soddisfazione_num']])            
            logistic_model = LogisticRegression(solver='liblinear', random_state=42)
            logistic_model.fit(X, y)
            beta = logistic_model.coef_[0][0]
            intercept = logistic_model.intercept_[0]
            p_pred = logistic_model.predict_proba(X)[:, 1]
            X_design = np.column_stack([np.ones(len(X)), X.flatten()])
            W = np.diag(p_pred * (1 - p_pred))
            try:
                fisher_info = X_design.T @ W @ X_design
                var_covar_matrix = np.linalg.inv(fisher_info)
                se_intercept = np.sqrt(var_covar_matrix[0, 0])
                se_beta = np.sqrt(var_covar_matrix[1, 1])
                correlation_coef = var_covar_matrix[0, 1] / (se_intercept * se_beta)
            except np.linalg.LinAlgError:
                print("Matrice singolare - utilizzando bootstrap per SE")
                se_beta = self._bootstrap_logistic_se(X, y)
                se_intercept = self._bootstrap_logistic_se_intercept(X, y)
                correlation_coef = np.nan
            z_score_beta = beta / se_beta
            z_score_intercept = intercept / se_intercept
            p_value_beta = 2 * (1 - norm.cdf(abs(z_score_beta)))
            p_value_intercept = 2 * (1 - norm.cdf(abs(z_score_intercept)))
            # odds ratio e intervalli di confidenza
            odds_ratio = np.exp(beta)
            z_critical = norm.ppf(0.975)
            beta_ci_lower = beta - z_critical * se_beta
            beta_ci_upper = beta + z_critical * se_beta
            or_ci_lower = np.exp(beta_ci_lower)
            or_ci_upper = np.exp(beta_ci_upper)
            # pseudo R-squared (mcfadden)
            pseudo_r2 = self._calculate_mcfadden_r2(logistic_model, X, y)
            log_likelihood = self._calculate_log_likelihood(logistic_model, X, y)            
            n_params = 2 
            n_obs = len(y)
            aic = 2 * n_params - 2 * log_likelihood
            bic = np.log(n_obs) * n_params - 2 * log_likelihood
            predictions = logistic_model.predict(X)
            accuracy = np.mean(predictions == y)
            confusion_matrix = self._create_confusion_matrix(y, predictions)
            self._assess_test_appropriateness(
                'Regressione Logistica',
                'Validazione crociata Digital Satisfaction Paradox',
                True,
                'Robusto per qualsiasi dimensione campionaria, appropriato per outcome binario'
            )
            return {
                'model_available': True,
                'n_observations': n_obs,
                'converged': True,
                'beta': beta,
                'se_beta': se_beta,
                'z_score': z_score_beta,
                'p_value': p_value_beta,
                'beta_ci_95': [beta_ci_lower, beta_ci_upper],
                'intercept': intercept,
                'se_intercept': se_intercept,
                'z_score_intercept': z_score_intercept,
                'p_value_intercept': p_value_intercept,
                'odds_ratio': odds_ratio,
                'or_ci_95': [or_ci_lower, or_ci_upper],
                'exp_beta': odds_ratio,
                'pseudo_r2_mcfadden': pseudo_r2,
                'log_likelihood': log_likelihood,
                'aic': aic,
                'bic': bic,
                'accuracy': accuracy,
                'correlation_coef_inter_beta': correlation_coef,
                'confusion_matrix': confusion_matrix,
                'significant_at_05': p_value_beta < 0.05,
                'significant_at_01': p_value_beta < 0.01,
                'significant_at_001': p_value_beta < 0.001,
                'model_equation': f'logit(P(Y=1)) = {intercept:.3f} + {beta:.3f} √ó Webinar'
            }
        except ImportError:
            self._log_methodological_warning(
                'Regressione Logistica',
                'sklearn non disponibile - analisi di validazione non eseguita'
            )
            return {'model_available': False, 'error': 'sklearn not available'}
        except Exception as e:
            self._log_methodological_warning(
                'Regressione Logistica', 
                f'Errore durante il calcolo: {str(e)}'
            )
            return {'model_available': False, 'error': str(e)}
    def _bootstrap_logistic_se(self, X, y, n_bootstrap=1000):
        """
        Calcola l'errore standard del coefficiente beta attraverso bootstrap.
        """
        try:            
            betas = []
            n = len(y)
            for _ in range(n_bootstrap):
                # campionamento bootstrap
                indices = np.random.choice(n, n, replace=True)
                X_boot = X[indices]
                y_boot = y[indices]
                try:
                    log_reg_boot = LogisticRegression(solver='liblinear', max_iter=1000)
                    log_reg_boot.fit(X_boot, y_boot)
                    betas.append(log_reg_boot.coef_[0][0])
                except:
                    continue
            return np.std(betas) if betas else 0.5  # Fallback
        except:
            return 0.5
    
    def _bootstrap_logistic_se_intercept(self, X, y, n_bootstrap=1000):
        """
        Calcola l'errore standard dell'intercetta attraverso bootstrap.
        """
        try:
            intercepts = []
            n = len(y)
            for _ in range(n_bootstrap):
                indices = np.random.choice(n, n, replace=True)
                X_boot = X[indices]
                y_boot = y[indices]
                try:
                    log_reg_boot = LogisticRegression(solver='liblinear', max_iter=1000)
                    log_reg_boot.fit(X_boot, y_boot)
                    intercepts.append(log_reg_boot.intercept_[0])
                except:
                    continue
            return np.std(intercepts) if intercepts else 0.5
        except:
            return 0.5
    
    def _calculate_mcfadden_r2(self, model, X, y):
        """
        Calcola il Pseudo R-squared di McFadden.
        """
        try:            
            null_model = LogisticRegression(solver='liblinear')
            null_model.fit(np.ones((len(X), 1)), y)
            # log-likelihood del modello null e completo
            ll_null = self._calculate_log_likelihood(null_model, np.ones((len(X), 1)), y)
            ll_model = self._calculate_log_likelihood(model, X, y)
            pseudo_r2 = 1 - (ll_model / ll_null)
            return max(0, pseudo_r2)  # ReLu
        except:
            return np.nan
    
    def _calculate_log_likelihood(self, model, X, y):
        """
        Calcola la log-likelihood del modello.
        """
        try:
            p_pred = model.predict_proba(X)[:, 1]
            p_pred = np.clip(p_pred, 1e-15, 1 - 1e-15)
            ll = np.sum(y * np.log(p_pred) + (1 - y) * np.log(1 - p_pred))  
            return ll
        except:
            return np.nan
    
    def _create_confusion_matrix(self, y_true, y_pred):
        """
        Crea matrice di confusione per classificazione binaria.
        """
        tp = np.sum((y_true == 1) & (y_pred == 1))  # True Positives
        tn = np.sum((y_true == 0) & (y_pred == 0))  # True Negatives
        fp = np.sum((y_true == 0) & (y_pred == 1))  # False Positives
        fn = np.sum((y_true == 1) & (y_pred == 0))  # False Negatives
        
        return {
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,
            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,
            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0
        }
    
    def _analyze_generational_membership_gap(self) -> dict:
        """Analizza il Generational Membership Gap."""
        age_groups = ['14-30', '31-50', '51-70', '>70']
        engagement_data = []
        
        for age in age_groups:
            cohort = self.df[self.df['eta_std'] == age]
            
            if len(cohort) > 0:
                membership_rate = (cohort['socio_std'] == 'S√¨').mean()
                avg_satisfaction = cohort['soddisfazione_num'].mean()
                engagement_index = membership_rate * avg_satisfaction

                cohort['cognitive_engagement'] = (
                    cohort['Approfondimenti'].notna().astype(int) + 
                    cohort['Proposte'].notna().astype(int)
                )
                cognitive_eng_mean = cohort['cognitive_engagement'].mean()
                engagement_data.append({
                    'age_group': age,
                    'cohort_size': len(cohort),
                    'membership_rate': membership_rate,
                    'avg_satisfaction': avg_satisfaction,
                    'engagement_index': engagement_index,
                    'cognitive_engagement_mean': cognitive_eng_mean,
                    'conversion_potential': (1 - membership_rate) * avg_satisfaction * len(cohort)
                })
        #inizializzazione lista rappresentativa di range et√† disponibili. Valori scelti in maniera aleatoria ma rappresentativa
        age_numerical = [22, 40, 60, 75]
        membership_rates = [d['membership_rate'] for d in engagement_data]
        correlation_spearman, p_value_spearman = spearmanr(age_numerical, membership_rates)
        correlation_pearson, p_value_pearson = pearsonr(age_numerical, membership_rates)   #pearson usato solo per confrontare
        sample_size = len(age_numerical)
        normality_feasible = sample_size >= 10  # Soglia minima per test normalit√†
        self._assess_test_appropriateness(
            'Correlazione di Spearman',
            f'Correlazione et√†-membership con n={sample_size} coorti',
            True,  # sempre appropriata
            'Non-parametrica, appropriata per campioni piccoli e dati ordinali'
        )
        self._assess_test_appropriateness(
            'Correlazione di Pearson',
            f'Correlazione et√†-membership con n={sample_size} coorti',
            normality_feasible,
            f'Campione troppo piccolo (n={sample_size}) per verificare normalit√† bivariata' if not normality_feasible else 'Appropriata con verifica normalit√†'
        )
        if not normality_feasible:
            self._log_methodological_warning(
                'Correlazione et√†-membership',
                f'Campione molto piccolo (n={sample_size}), preferire metodi non-parametrici'
            )
        return {
            'engagement_metrics': engagement_data,
            'age_membership_correlation': correlation_spearman,
            'correlation_p_value': p_value_spearman,
            'r_squared': correlation_spearman**2,
            'spearman_correlation': correlation_spearman,
            'spearman_p_value': p_value_spearman,
            'pearson_correlation': correlation_pearson,
            'pearson_p_value': p_value_pearson,
            'sample_size': sample_size,
            'normality_test_feasible': normality_feasible,
            'recommended_method': 'Spearman'
        }

    def _analyze_inverse_contentment_effect(self) -> dict:
        """Analizza l'Inverse Contentment Effect."""
        def has_elaborate_feedback(row):
            feedback_fields = ['Approfondimenti', 'Proposte']
            for field in feedback_fields:
                if field in row and pd.notna(row[field]):
                    text = str(row[field]).strip()
                    if (text != 'N/A' and len(text) > 10 and 
                        'nessun' not in text.lower() and 'niente' not in text.lower()):
                        return True
            return False
        
        self.df['has_feedback'] = self.df.apply(has_elaborate_feedback, axis=1)
        satisfied = self.df[self.df['soddisfazione_num'] == 2]
        very_satisfied = self.df[self.df['soddisfazione_num'] == 3]
        feedback_metrics = {
            'satisfied': {
                'total': len(satisfied),
                'with_feedback': satisfied['has_feedback'].sum(),
                'feedback_rate': satisfied['has_feedback'].mean()
            },
            'very_satisfied': {
                'total': len(very_satisfied),
                'with_feedback': very_satisfied['has_feedback'].sum(),
                'feedback_rate': very_satisfied['has_feedback'].mean()
            }
        }
        contingency_table = [
            [feedback_metrics['satisfied']['with_feedback'],
             feedback_metrics['satisfied']['total'] - feedback_metrics['satisfied']['with_feedback']],
            [feedback_metrics['very_satisfied']['with_feedback'],
             feedback_metrics['very_satisfied']['total'] - feedback_metrics['very_satisfied']['with_feedback']]
        ]
        
        # VERIFICA CRITERI DI COCHRAN
        chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)
        min_expected = np.min(expected)
        cells_below_5 = np.sum(expected < 5)
        total_cells = expected.size
        cochran_satisfied = (cells_below_5 == 0) or (cells_below_5 <= 0.2 * total_cells and min_expected >= 1)
        # se cochran violato usare fisher
        if not cochran_satisfied:
            fisher_odds, fisher_p = stats.fisher_exact(contingency_table)
            primary_test = 'Fisher Exact Test'
            primary_p_value = fisher_p
            self._assess_test_appropriateness(
                'Test Chi-quadro',
                f'Tabella 2√ó2 con min freq. attesa = {min_expected:.1f}',
                False,
                f'Frequenze attese < 5 in {cells_below_5}/{total_cells} celle, viola criteri Cochran'
            )
            self._assess_test_appropriateness(
                'Fisher Exact Test',
                'Tabella 2√ó2 con frequenze attese basse',
                True,
                'Appropriato per celle con frequenze basse, nessuna assunzione violata'
            )
        else:
            fisher_odds, fisher_p = np.nan, np.nan
            primary_test = 'Chi-quadro'
            primary_p_value = p_value
            self._assess_test_appropriateness(
                'Test Chi-quadro',
                f'Tabella 2√ó2 con min freq. attesa = {min_expected:.1f}',
                True,
                'Criteri di Cochran soddisfatti, appropriato per tabelle di contingenza'
            )
        contentment_ratio = (feedback_metrics['very_satisfied']['feedback_rate'] / 
                           feedback_metrics['satisfied']['feedback_rate'])
        return {
            'feedback_metrics': feedback_metrics,
            'contentment_ratio': contentment_ratio,
            'contingency_table': contingency_table,
            # test statistici
            'primary_test': primary_test,
            'primary_p_value': primary_p_value,
            'chi2_statistic': chi2_stat,
            'chi2_p_value': p_value,
            'fisher_odds_ratio': fisher_odds,
            'fisher_p_value': fisher_p,
            'cochran_criteria': {
                'satisfied': cochran_satisfied,
                'min_expected_frequency': min_expected,
                'cells_below_5': int(cells_below_5),
                'total_cells': int(total_cells),
                'expected_frequencies': expected.tolist()
            },
            'effect_confirmed': contentment_ratio > 1,
            'p_value': primary_p_value
        }
    
    def calculate_cultural_engagement_score(self) -> dict:
        """
        Calcola il Cultural Engagement Score (CES) e le sue componenti. v2!!!
        returns:
        --------
        dict
            Componenti e score finale del CES
        """
        print("...Calcolo Cultural Engagement Score...")
        avg_satisfaction = self.df['soddisfazione_num'].mean()
        satisfaction_component = (avg_satisfaction - 1) / 2  # normalizzazione 0-1
        membership_rate = (self.df['socio_std'] == 'S√¨').mean()
        self.df['cognitive_engagement'] = (
            self.df['Approfondimenti'].notna().astype(int) + 
            self.df['Proposte'].notna().astype(int)
        )
        cognitive_engagement_mean = self.df['cognitive_engagement'].mean()/2 # normalizzazione 0-1
        # Formula CES: (S+1/2) √ó sqrt(1 + M) √ó sqrt(1 + CEI)
        ces_score = satisfaction_component * (1 + membership_rate)**0.5 * (1 + cognitive_engagement_mean)**0.5
        max_satisfaction = 1.0
        max_membership = 1.0
        max_cognitive = 1.0
        maximum_ces = max_satisfaction * (1 + max_membership)**0.5 * (1 + max_cognitive)**0.5
        bootstrap_scores = self._bootstrap_ces_validation(n_iterations=1000)
        ces_results = {
            'score': ces_score,
            'components': {
                'satisfaction': satisfaction_component,
                'membership': membership_rate,
                'cognitive_engagement_mean': cognitive_engagement_mean,
                'max_theoretical': maximum_ces
            },
            'percentile_rank': (ces_score / maximum_ces) * 100,
            'bootstrap_validation': {
                'mean': np.mean(bootstrap_scores),
                'std_error': np.std(bootstrap_scores),
                'ci_95_lower': np.percentile(bootstrap_scores, 2.5),
                'ci_95_upper': np.percentile(bootstrap_scores, 97.5),
                'cv': np.std(bootstrap_scores) / np.mean(bootstrap_scores)
            }
        }
        ces_results['improvement_scenarios'] = self._generate_improvement_scenarios(ces_results['components'])
        self.results['ces'] = ces_results
        return ces_results
    
    def _bootstrap_ces_validation(self, n_iterations: int = 1000) -> list:
        """Validazione bootstrap del CES."""
        bootstrap_scores = []
        for _ in range(n_iterations):
            rng = np.random.RandomState()
            sample = self.df.sample(n=len(self.df), replace=True, random_state=rng)            
            avg_sat = sample['soddisfazione_num'].mean()
            sat_comp = (avg_sat - 1 ) / 2
            mem_rate = (sample['socio_std'] == 'S√¨').mean()
            cog_rate = (sample['Approfondimenti'].notna().astype(int) + 
                        sample['Proposte'].notna().astype(int)
                        ).mean() / 2
            
            ces = sat_comp * (1 + mem_rate)**0.5 * (1 + cog_rate)**0.5
            bootstrap_scores.append(ces)
        return bootstrap_scores
    
    def _generate_improvement_scenarios(self, baseline_components: dict) -> dict:
        """Genera scenari di miglioramento per il CES."""
        scenarios = {
            'conservativo': {'membership': baseline_components['membership']+0.05, 'cognitive_engagement_mean': baseline_components['cognitive_engagement_mean']+0.10},
            'ambizioso': {'membership': baseline_components['membership']+0.15, 'cognitive_engagement_mean': baseline_components['cognitive_engagement_mean']+0.25},
            'eccellenza': {'membership': baseline_components['membership']+0.25, 'cognitive_engagement_mean': baseline_components['cognitive_engagement_mean']+0.40}
        }
        
        results = {}
        baseline_ces = (baseline_components['satisfaction'] * 
                       (1 + baseline_components['membership']) * 
                       (1 + baseline_components['cognitive_engagement_mean']))
        
        for scenario, changes in scenarios.items():
            new_membership = min(1.0, baseline_components['membership'] + changes['membership'])
            new_cognitive = min(1.0, baseline_components['cognitive_engagement_mean'] + changes['cognitive_engagement_mean'])
            new_ces = (baseline_components['satisfaction'] * 
                      (1 + new_membership)**0.5 * 
                      (1 + new_cognitive)**0.5)
            improvement = ((new_ces - baseline_ces) / baseline_ces) * 100
            results[scenario] = {
                'projected_ces': new_ces,
                'improvement_percentage': improvement,
                'target_membership': new_membership,
                'target_cognitive': new_cognitive
            }
        return results
    
    def print_detailed_digital_paradox_analysis(self):
        """
        Stampa analisi dettagliata del Digital Satisfaction Paradox con tutti i test.
        """
        print("\n" + "="*90)
        print("DIGITAL SATISFACTION PARADOX - ANALISI COMPLETA MULTI-METODOLOGICA")
        print("="*90)
        
        if 'digital_paradox' not in self.results.get('key_findings', {}):
            print("XX Analisi non ancora eseguita XX")
            return
        paradox_data = self.results['key_findings']['digital_paradox']
        # === STATISTICHE DESCRITTIVE ===
        print("STATISTICHE DESCRITTIVE:")
        print(f"\tCARTACEO:")
        print(f"\t- N = {paradox_data['cartaceo_stats']['n']}")
        print(f"\t- Media = {paradox_data['cartaceo_stats']['mean']:.3f} ¬± {paradox_data['cartaceo_stats']['std']:.3f}")
        print(f"\t- % Molto Soddisfatti = {paradox_data['cartaceo_stats']['satisfaction_rate_max']:.1%}")
        print(f"\tWEBINAR:")
        print(f"\t- N = {paradox_data['webinar_stats']['n']}")
        print(f"\t- Media = {paradox_data['webinar_stats']['mean']:.3f} ¬± {paradox_data['webinar_stats']['std']:.3f}")
        print(f"\t- % Molto Soddisfatti = {paradox_data['webinar_stats']['satisfaction_rate_max']:.1%}")
        print(f"\tDIFFERENZA: +{paradox_data['difference']:.3f} punti (Webinar > Cartaceo)")

        # === FISHER EXACT TEST (PRINCIPALE) ===
        print(f"\nFISHER EXACT TEST (Metodo Principale):")
        print(f"\t- Odds Ratio: {paradox_data['fisher_odds_ratio']:.3f}")
        print(f"\t- p-value: {paradox_data['fisher_p_value']:.4f}")
        print(f"\t- Significativit√†: {'***' if paradox_data['fisher_p_value'] < 0.001 else '**' if paradox_data['fisher_p_value'] < 0.01 else '*' if paradox_data['fisher_p_value'] < 0.05 else 'n.s.'}")
        print(f"\t- Differenza Proporzioni: +{paradox_data['proportion_difference']:.3f}")
        print(f"\t- Appropriatezza: OTTIMALE per dati binari")
        contingency = paradox_data['contingency_table']
        print(f"\tTabella di Contingenza 2√ó2:")
        print(f"                    Soddisfatto(2)  Molto Sodd.(3)")
        print(f"      Cartaceo:     {contingency[0][0]:>11}  {contingency[0][1]:>13}")
        print(f"      Webinar:      {contingency[1][0]:>11}  {contingency[1][1]:>13}")
        # === REGRESSIONE LOGISTICA ===
        if paradox_data['logistic_regression'].get('model_available', False):
            print(f"\nREGRESSIONE LOGISTICA (Validazione Terziaria):")
            lr_data = paradox_data['logistic_regression']
            print(f"\tCOEFFICIENTI:")
            print(f"\t- Œ≤ (Webinar): {lr_data['beta']:.3f} ¬± {lr_data['se_beta']:.3f}")
            print(f"\t- Z-score: {lr_data['z_score']:.3f}")
            print(f"\t- p-value: {lr_data['p_value']:.4f}")
            print(f"\t- Significativit√†: {'***' if lr_data['p_value'] < 0.001 else '**' if lr_data['p_value'] < 0.01 else '*' if lr_data['p_value'] < 0.05 else 'n.s.'}")
            print(f"\t- CI 95% (Œ≤): [{lr_data['beta_ci_95'][0]:.3f}, {lr_data['beta_ci_95'][1]:.3f}]")
            
            print(f"\tODDS RATIO:")
            print(f"\t- OR [exp(Œ≤)]: {lr_data['odds_ratio']:.3f}")
            print(f"\t- CI 95% (OR): [{lr_data['or_ci_95'][0]:.3f}, {lr_data['or_ci_95'][1]:.3f}]")
            print(f"\t- Incremento Chance: +{(lr_data['odds_ratio']-1)*100:.1f}%")

            print(f"\tQUALIT√Ä MODELLO:")
            print(f"\t- Pseudo R¬≤ (McFadden): {lr_data['pseudo_r2_mcfadden']:.3f}")
            print(f"\t- Log-Likelihood: {lr_data['log_likelihood']:.1f}")
            print(f"\t- AIC: {lr_data['aic']:.1f}")
            print(f"\t- BIC: {lr_data['bic']:.1f}")
            print(f"\t- Accuracy: {lr_data['accuracy']:.3f}")

            print(f"\tEQUAZIONE:")
            print(f"\t{lr_data['model_equation']}")

            or_diff = abs(paradox_data['fisher_odds_ratio'] - lr_data['odds_ratio'])
            convergence_status = "ECCELLENTE" if or_diff < 0.1 else "ACCETTABILE" if or_diff < 0.5 else "PROBLEMATICA"
            print(f"\tCONVERGENZA METODOLOGICA:")
            print(f"\t- Fisher OR: {paradox_data['fisher_odds_ratio']:.3f}")
            print(f"\t- Logistic OR: {lr_data['odds_ratio']:.3f}")
            print(f"\t- Differenza: {or_diff:.3f}")
            print(f"\t- Status: {convergence_status}")
        else:
            print(f"\n REGRESSIONE LOGISTICA: Non disponibile")
            if 'error' in paradox_data['logistic_regression']:
                print(f"\tErrore: {paradox_data['logistic_regression']['error']}")
        
        # === MANN-WHITNEY U (SECONDARIO CON DISCLAIMER) ===
        print(f"\nMANN-WHITNEY U (Analisi Secondaria - LIMITAZIONI):")
        print(f"\t-U-statistic: {paradox_data['mannwhitney_u']:.1f}")
        print(f"\t-p-value: {paradox_data['mannwhitney_p']:.4f}")
        print(f"\t-Rosenthal r: {paradox_data['rosenthal_r']:.3f}")
        print(f"\t-Z-score: {paradox_data['z_score']:.3f}")
        print(f"\t-Ties: {paradox_data['ties_percentage']:.1%} (ECCESSIVI)")
        print(f"\t-Limitazioni: Solo 2 valori distinti, violazione assunzioni")
        
        # === ANALISI ET√Ä ===
        print(f"\nAGE PARADOX:")
        print(f"\t- Et√† media Webinar: {paradox_data['age_webinar']:.1f} anni")
        print(f"\t- Et√† media Cartaceo: {paradox_data['age_cartaceo']:.1f} anni") 
        print(f"\t- Differenza: +{paradox_data['age_paradox']:.1f} anni (Webinar > Cartaceo)")
        print(f"\t- Interpretazione: Controintuitiva rispetto al digital divide atteso")
        
        # === RACCOMANDAZIONI METODOLOGICHE ===
        print(f"\nRACCOMANDAZIONI METODOLOGICHE:")
        print(f"\t UTILIZZARE: Fisher Exact Test come risultato principale")
        print(f"\t UTILIZZARE: Regressione Logistica per validazione e IC")
        print(f"\t EVITARE: Mann-Whitney U per dati con ties eccessivi")
        print(f"\t RIPORTARE: Odds Ratio e intervalli di confidenza al 95%")
        print(f"\t CITARE: Tabella di contingenza e proporzioni per trasparenza")

        print("="*90)
    
    def create_visualizations(self):
        """Crea tutte le visualizzazioni per l'analisi."""
        print("Generazione visualizzazioni...")
        self._plot_demographic_analysis()
        self._plot_digital_satisfaction_paradox()
        self._plot_generational_membership_gap()
        self._plot_satisfaction_distribution()
        self._plot_satisfaction_elements()
        self._plot_communication_channels()
        self._plot_ces_analysis()
        self._plot_demographic_overview_panel()
        self._plot_participation_modalities_panel()
        print(f"Grafici salvati in: {self.output_dir}")
    
    def _plot_demographic_overview_panel(self):
        """
        - sx: Distribuzione per fascia d'et√† 
        - cen: Ripartizione soci vs non-soci
        - dx: Livello di scolarizzazione
        """
        fig, axes = plt.subplots(1, 3, figsize=(20, 6))
        fig.suptitle('Panoramica Demografica del Campione Primario', fontsize=16, fontweight='bold', 
                     color=PURPLE_PALETTE['dark_purple'])
        # 1. DISTRIBUZIONE PER FASCIA D'ET√Ä (Sinistra)
        age_counts = self.df['eta_std'].value_counts()
        age_order = ['14-30', '31-50', '51-70', '>70']
        age_counts = age_counts.reindex(age_order, fill_value=0)
        bars1 = axes[0].bar(age_counts.index, age_counts.values, 
                           color=PURPLE_PALETTE['primary_purple'], alpha=0.8,
                           edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[0].set_title('Distribuzione per Fascia d\'Et√†', 
                         color=PURPLE_PALETTE['dark_purple'], fontweight='bold')
        axes[0].set_ylabel('Numero Partecipanti')
        axes[0].set_xlabel('Fascia d\'Et√†')
        axes[0].tick_params(axis='x', rotation=45, colors=PURPLE_PALETTE['dark_purple'])
        axes[0].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        for bar in bars1:
            height = bar.get_height()
            axes[0].text(bar.get_x() + bar.get_width()/2, height + 0.5,
                        f'{int(height)}', ha='center', va='bottom', fontweight='bold',
                        color=PURPLE_PALETTE['dark_purple'])
        # 2. RIPARTIZIONE SOCI VS NON-SOCI (Centro)
        member_counts = self.df['socio_std'].value_counts()
        member_order = ['No', 'S√¨']
        member_counts = member_counts.reindex(member_order, fill_value=0)
        colors_pie = [PURPLE_PALETTE['sage'], PURPLE_PALETTE['dusty_rose']]
        labels_pie = ['Non Soci', 'Soci']
        wedges, texts, autotexts = axes[1].pie(member_counts.values, 
                                              labels=labels_pie,
                                              autopct='%1.1f%%', colors=colors_pie, 
                                              startangle=90, 
                                              textprops={'color': PURPLE_PALETTE['dark_purple'], 'fontweight': 'bold'})
        axes[1].set_title('Ripartizione Soci vs Non-Soci', 
                         color=PURPLE_PALETTE['dark_purple'], fontweight='bold')
        # 3. LIVELLO DI SCOLARIZZAZIONE (Destra)
        education_counts = self.df['Titolo di studio'].value_counts()
        education_mapping = {
            'Licenza media': 'Licenza media',
            'Diploma di scuola superiore': 'Diploma superiore', 
            'Laurea': 'Laurea',
            'Laurea magistrale': 'Laurea magistrale',
            'Post-laurea': 'Post-laurea'
        }
        education_clean = {}
        for original, mapped in education_mapping.items():
            if original in education_counts:
                education_clean[mapped] = education_counts[original]
        education_order = ['Licenza media', 'Diploma superiore', 'Laurea', 'Laurea magistrale', 'Post-laurea']
        ordered_education = {k: education_clean.get(k, 0) for k in education_order if k in education_clean}
        bars3 = axes[2].barh(list(ordered_education.keys()), list(ordered_education.values()),
                            color=PURPLE_PALETTE['warm_gold'], alpha=0.8,
                            edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[2].set_title('Livello di Scolarizzazione', 
                         color=PURPLE_PALETTE['dark_purple'], fontweight='bold')
        axes[2].set_xlabel('Numero Partecipanti')
        axes[2].grid(axis='x', alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        for bar in bars3:
            width = bar.get_width()
            if width > 0:  # Solo se c'√® valore
                percentage = (width / sum(list(ordered_education.values()))) * 100
                axes[2].text(width + 0.5, bar.get_y() + bar.get_height()/2,
                            f'{int(width)} ({percentage:.1f}%)', ha='left', va='center', fontweight='bold',
                            color=PURPLE_PALETTE['dark_purple'])
        total_participants = len(self.df)
        dominant_age = age_counts.idxmax()
        dominant_age_count = age_counts.max()
        soci_percentage = (self.df['socio_std'] == 'S√¨').mean() * 100
        top_education = max(ordered_education, key=ordered_education.get) if ordered_education else 'N/A'
        stats_text = f'Campione totale: {total_participants} partecipanti\n'
        stats_text += f'Fascia et√† dominante: {dominant_age} ({dominant_age_count} partecipanti)\n'
        stats_text += f'Percentuale soci: {soci_percentage:.1f}%\n'
        stats_text += f'Livello istruzione prevalente: {top_education}'
        fig.text(0.02, 0.02, stats_text, fontsize=9, 
                 bbox=dict(boxstyle='round', facecolor=PURPLE_PALETTE['lavender'], 
                          alpha=0.9, edgecolor=PURPLE_PALETTE['primary_purple']),
                 color=PURPLE_PALETTE['dark_purple'])
        plt.tight_layout()
        plt.savefig(self.output_dir / 'demographic_overview_panel.png', 
                   facecolor='white', edgecolor='none', dpi=300, bbox_inches='tight')
        plt.close()
    def _plot_participation_modalities_panel(self):
        """
        - sx: Modalit√† di fruizione (Cartaceo vs Webinar)
        - dx: Partecipazione per argomento trattato
        """
        fig, axes = plt.subplots(1, 2, figsize=(16, 8))
        fig.suptitle('Modalit√† di Partecipazione e Distribuzione per Argomenti', 
                     fontsize=16, fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        
        # 1. MODALIT√Ä DI FRUIZIONE (Sinistra)
        modality_counts = self.df['Fonte'].value_counts()
        colors_modality = [PURPLE_PALETTE['periwinkle'], PURPLE_PALETTE['royal_purple']]
        
        bars1 = axes[0].bar(modality_counts.index, modality_counts.values,
                           color=colors_modality, alpha=0.8,
                           edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[0].set_title('Modalit√† di Fruizione', 
                         color=PURPLE_PALETTE['dark_purple'], fontweight='bold')
        axes[0].set_ylabel('Numero Partecipanti')
        axes[0].set_xlabel('Modalit√†')
        axes[0].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        total_modality = sum(modality_counts.values)
        for bar, count in zip(bars1, modality_counts.values):
            percentage = (count / total_modality) * 100
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                        f'{count}\n({percentage:.1f}%)', ha='center', va='bottom', 
                        fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        # 2. PARTECIPAZIONE PER ARGOMENTO (Destra)
        topic_counts = self.df['Argomento'].value_counts()
        topic_counts = topic_counts.sort_values(ascending=True)
        colors_topics = [PURPLE_SEQUENTIAL[i % len(PURPLE_SEQUENTIAL)] for i in range(len(topic_counts))]
        bars2 = axes[1].barh(range(len(topic_counts)), topic_counts.values,
                            color=colors_topics, alpha=0.8,
                            edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[1].set_yticks(range(len(topic_counts)))
        axes[1].set_yticklabels(topic_counts.index, fontsize=10)
        axes[1].set_title('Partecipazione per Argomento Trattato', 
                         color=PURPLE_PALETTE['dark_purple'], fontweight='bold')
        axes[1].set_xlabel('Numero Partecipanti')
        axes[1].grid(axis='x', alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        for i, (bar, count) in enumerate(zip(bars2, topic_counts.values)):
            axes[1].text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
                        f'{count}', ha='left', va='center', fontweight='bold',
                        color=PURPLE_PALETTE['dark_purple'])

        digital_adoption = (modality_counts.get('Webinar', 0) / total_modality) * 100
        most_popular_topic = topic_counts.index[-1] 
        participation_stats = f'STATISTICHE PARTECIPAZIONE:\n'
        participation_stats += f'- Adozione digitale: {digital_adoption:.1f}%\n'
        participation_stats += f'- Argomento pi√π seguito: {most_popular_topic}\n'
        participation_stats += f'- Range partecipazione: {topic_counts.min()}-{topic_counts.max()} per evento\n'
        participation_stats += f'- Eventi monitorati: {len(topic_counts)}'
        axes[1].text(0.98, 0.02, participation_stats, transform=axes[1].transAxes,
                    bbox=dict(boxstyle='round', facecolor=PURPLE_PALETTE['lavender'], 
                             alpha=0.9, edgecolor=PURPLE_PALETTE['sage'], linewidth=2),
                    color=PURPLE_PALETTE['dark_purple'], fontsize=9, 
                    ha='right', va='bottom')
        plt.tight_layout()
        plt.savefig(self.output_dir / 'participation_modalities_panel.png', 
                   facecolor='white', edgecolor='none', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_demographic_analysis(self):
        """Crea visualizzazioni demografiche."""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Analisi Demografica del Campione', fontsize=16, fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        # 1. Distribuzione per et√†
        age_counts = self.df['eta_std'].value_counts()
        bars1 = axes[0,0].bar(age_counts.index, age_counts.values, 
                             color=PURPLE_PALETTE['primary_purple'], alpha=0.8, 
                             edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[0,0].set_title('Distribuzione per Fascia d\'Et√†', color=PURPLE_PALETTE['dark_purple'])
        axes[0,0].set_ylabel('Numero Partecipanti')
        axes[0,0].tick_params(axis='x', rotation=45, colors=PURPLE_PALETTE['dark_purple'])
        axes[0,0].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        # 2. Soci vs Non-soci
        member_counts = self.df['socio_std'].value_counts()
        colors_pie = [PURPLE_PALETTE['light_purple'], PURPLE_PALETTE['sage']]
        wedges, texts, autotexts = axes[0,1].pie(member_counts.values, labels=member_counts.index, 
                                                autopct='%1.1f%%', colors=colors_pie, 
                                                startangle=90, textprops={'color': PURPLE_PALETTE['dark_purple']})
        axes[0,1].set_title('Distribuzione Soci vs Non-Soci', color=PURPLE_PALETTE['dark_purple'])
        # 3. Soddisfazione per et√†
        satisfaction_by_age = self.df.groupby('eta_std')['soddisfazione_num'].mean()
        bars3 = axes[1,0].bar(satisfaction_by_age.index, satisfaction_by_age.values, 
                             color=PURPLE_PALETTE['periwinkle'], alpha=0.8,
                             edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[1,0].set_title('Soddisfazione Media per Fascia d\'Et√†', color=PURPLE_PALETTE['dark_purple'])
        axes[1,0].set_ylabel('Soddisfazione Media')
        axes[1,0].set_ylim(2.0, 3.0)
        axes[1,0].tick_params(axis='x', rotation=45, colors=PURPLE_PALETTE['dark_purple'])
        axes[1,0].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        # 4. Modalit√† di fruizion
        mode_counts = self.df['Fonte'].value_counts()
        bars4 = axes[1,1].bar(mode_counts.index, mode_counts.values, 
                             color=PURPLE_PALETTE['warm_gold'], alpha=0.8,
                             edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[1,1].set_title('Distribuzione per Modalit√† di Fruizione', color=PURPLE_PALETTE['dark_purple'])
        axes[1,1].set_ylabel('Numero Partecipanti')
        axes[1,1].tick_params(axis='x', rotation=45, colors=PURPLE_PALETTE['dark_purple'])
        axes[1,1].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        plt.tight_layout()
        plt.savefig(self.output_dir / 'demographic_analysis.png', facecolor='white', edgecolor='none')
        plt.close()
    
    def _plot_digital_satisfaction_paradox(self):
        """
        Visualizza il Digital Satisfaction Paradox con statistiche corrette.
        """
        fig, axes = plt.subplots(1, 3, figsize=(20, 6))
        fig.suptitle('Paradosso di Soddisfazione Digitale', 
                     fontsize=16, fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        webinar_data = self.df[self.df['Fonte'] == 'Webinar']['soddisfazione_num']
        cartaceo_data = self.df[self.df['Fonte'] == 'Cartaceo']['soddisfazione_num']
        paradox_data = self.results['key_findings']['digital_paradox']
        prop_cartaceo = paradox_data['cartaceo_stats']['satisfaction_rate_max']
        prop_webinar = paradox_data['webinar_stats']['satisfaction_rate_max']
        modes = ['Cartaceo', 'Webinar']
        proportions = [prop_cartaceo, prop_webinar]
        bars1 = axes[0].bar(modes, proportions, 
                           color=[PURPLE_PALETTE['dusty_rose'], PURPLE_PALETTE['periwinkle']], 
                           alpha=0.8, edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[0].set_title('Proporzione "Molto Soddisfatti" per Modalit√†', 
                         color=PURPLE_PALETTE['dark_purple'])
        axes[0].set_ylabel('Proporzione "Molto Soddisfatti"')
        axes[0].set_ylim(0, 1.0)
        axes[0].grid(True, alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        fisher_text = f'FISHER EXACT TEST (Principale)\n'
        fisher_text += f'Odds Ratio = {paradox_data["fisher_odds_ratio"]:.2f}\n'
        fisher_text += f'p-value = {paradox_data["fisher_p_value"]:.3f}\n'
        fisher_text += f'Diff. Proporzioni = +{paradox_data["proportion_difference"]:.3f}\n'
        fisher_text += f'Test Raccomandato: ' + ('S√¨' if paradox_data["fisher_p_value"] < 0.05 else 'No')
        axes[0].text(0.02, 0.98, fisher_text, transform=axes[0].transAxes,
                    bbox=dict(boxstyle='round', facecolor=PURPLE_PALETTE['lavender'], 
                             alpha=0.9, edgecolor=PURPLE_PALETTE['sage'], linewidth=2),
                    color=PURPLE_PALETTE['dark_purple'], fontsize=9, va='top')
        for bar, prop in zip(bars1, proportions):
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                        f'{prop:.1%}', ha='center', va='bottom', fontweight='bold',
                        color=PURPLE_PALETTE['dark_purple'])
        # === 2. TABELLA DI CONTINGENZA VISUALIZZATA ===
        contingency = np.array(paradox_data['contingency_table'])
        sns.heatmap(contingency, annot=True, fmt='d', 
                   xticklabels=['Soddisfatto (2)', 'Molto Soddisfatto (3)'],
                   yticklabels=['Cartaceo', 'Webinar'],
                   cmap=sns.color_palette(PURPLE_SEQUENTIAL, as_cmap=True),
                   ax=axes[1], cbar_kws={'label': 'Numero Partecipanti'},
                   annot_kws={'color': PURPLE_PALETTE['dark_purple'], 'fontweight': 'bold'})
        axes[1].set_title('Tabella di Contingenza 2x2\n(Base per Fisher Exact Test)', 
                         color=PURPLE_PALETTE['dark_purple'])
        axes[1].set_xlabel('Livello Soddisfazione')
        axes[1].set_ylabel('Modalit√† Fruizione')
        # === 3. CONFRONTO METODOLOGIE + REGRESSIONE LOGISTICA ===
        methods = ['Fisher Exact\n(Principale)', 'Mann-Whitney U\n(Problematico)', 'Logistic Regr.\n(Validazione)']
        p_values = [
            paradox_data['fisher_p_value'], 
            paradox_data['mannwhitney_p'],
            paradox_data['logistic_regression'].get('p_value', 1.0) if paradox_data['logistic_regression'].get('model_available', False) else 1.0
        ]        
        colors_methods = [PURPLE_PALETTE['sage'], PURPLE_PALETTE['dusty_rose'], PURPLE_PALETTE['periwinkle']]
        bars3 = axes[2].bar(methods, p_values, color=colors_methods, alpha=0.8,
                           edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[2].axhline(y=0.05, color=PURPLE_PALETTE['dark_purple'], linestyle='--', 
                       linewidth=2, label='Œ± = 0.05')
        axes[2].set_title('Confronto Metodologie Statistiche', color=PURPLE_PALETTE['dark_purple'])
        axes[2].set_ylabel('p-value')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        odds_ratios = [
            paradox_data['fisher_odds_ratio'],
            np.nan,  # Mann-Whitney non ha OR
            paradox_data['logistic_regression'].get('odds_ratio', np.nan) if paradox_data['logistic_regression'].get('model_available', False) else np.nan
        ]
        for bar, p_val, or_val, method in zip(bars3, p_values, odds_ratios, methods):
            significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else "ns"
            or_text = f'\nOR: {or_val:.2f}' if not np.isnan(or_val) else ''
            axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,
                        f'{p_val:.3f}\n{significance}{or_text}', ha='center', va='bottom', 
                        fontweight='bold', color=PURPLE_PALETTE['dark_purple'], fontsize=9)
        if paradox_data['logistic_regression'].get('model_available', False):
            logistic_data = paradox_data['logistic_regression']
            logistic_text = f'REGRESSIONE LOGISTICA:\n'
            logistic_text += f'Œ≤ = {logistic_data.get("beta", 0):.3f} ¬± {logistic_data.get("se_beta", 0):.3f}\n'
            logistic_text += f'OR = {logistic_data.get("odds_ratio", 0):.3f}\n'
            logistic_text += f'CI 95%: [{logistic_data.get("or_ci_95", [0,0])[0]:.2f}, {logistic_data.get("or_ci_95", [0,0])[1]:.2f}]\n'
            logistic_text += f'Pseudo R¬≤ = {logistic_data.get("pseudo_r2_mcfadden", 0):.3f}\n'
            logistic_text += f'Convergenza: {logistic_data.get("convergence_status", "N/A")}\n'
            axes[2].text(0.02, 0.98, logistic_text, transform=axes[2].transAxes,
                        bbox=dict(boxstyle='round', facecolor=PURPLE_PALETTE['lavender'], 
                                 alpha=0.9, edgecolor=PURPLE_PALETTE['periwinkle'], linewidth=2),
                        color=PURPLE_PALETTE['dark_purple'], fontsize=8, va='top')
        if paradox_data.get('mw_methodological_warning', False):
            warning_text = f'Mann-Whitney U non ottimale:\n'
            warning_text += f'- Ties: {paradox_data["ties_percentage"]:.1%}\n'
            warning_text += f'- Solo 2 valori distinti\n'
            warning_text += f'- Dati essenzialmente binari'
            axes[2].text(0.98, 0.02, warning_text, transform=axes[2].transAxes,
                        bbox=dict(boxstyle='round', facecolor='#FFE4E1', 
                                 alpha=0.9, edgecolor='#DC143C', linewidth=1),
                        color='#8B0000', fontsize=8, ha='right', va='bottom')
        plt.tight_layout()
        plt.savefig(self.output_dir / 'digital_satisfaction_paradox_corrected.png', 
                   facecolor='white', edgecolor='none', dpi=300, bbox_inches='tight')
        contingency_df = pd.DataFrame(
            contingency,
            index=['Cartaceo', 'Webinar'],
            columns=['Soddisfatto (2)', 'Molto Soddisfatto (3)']
        )
        contingency_df.to_csv(self.output_dir / 'contingency_table_fisher_test.csv')
    
    def _plot_generational_membership_gap(self):
        """Visualizza il Generational Membership Gap"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Analisi del Divario Associazionistico Generazionale', fontsize=16, fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        gap_data = self.results['key_findings']['generational_gap']['engagement_metrics']
        age_groups = [d['age_group'] for d in gap_data]
        membership_rates = [d['membership_rate'] * 100 for d in gap_data]
        engagement_indices = [d['engagement_index'] for d in gap_data]
        cognitive_engagement = [d['cognitive_engagement_mean'] for d in gap_data]
        complex_engagement = [(cognitive + engagement) / 2 for cognitive, engagement in zip(cognitive_engagement, engagement_indices)]
        # 1. Percentuale soci per et√†
        bars1 = axes[0,0].bar(age_groups, membership_rates, 
                             color=PURPLE_PALETTE['royal_purple'], alpha=0.8,
                             edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[0,0].set_title('Percentuale Soci per Fascia d\'Et√†', color=PURPLE_PALETTE['dark_purple'])
        axes[0,0].set_ylabel('% Soci')
        axes[0,0].tick_params(axis='x', rotation=45, colors=PURPLE_PALETTE['dark_purple'])
        axes[0,0].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        corr = self.results['key_findings']['generational_gap']['age_membership_correlation']
        axes[0,0].text(0.02, 0.98, f'r = {corr:.3f}', transform=axes[0,0].transAxes,
                      bbox=dict(boxstyle='round', facecolor=PURPLE_PALETTE['lavender'], 
                               alpha=0.9, edgecolor=PURPLE_PALETTE['primary_purple']),
                      color=PURPLE_PALETTE['dark_purple'])
        # 2. Engagement Index
        bar_width = 0.25
        axes[0,1].set(ylim=(0, 2.0))
        x_positions = np.arange(len(age_groups))
        bars_engagement = axes[0,1].bar(x_positions - bar_width, engagement_indices, 
                width=bar_width, color=PURPLE_PALETTE['sage'], alpha=0.8,
                edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5, label='Institutional Engagement Index (IEI)')
        bars_cognitive = axes[0,1].bar(x_positions, cognitive_engagement, 
                   width=bar_width, color=PURPLE_PALETTE['royal_purple'], alpha=0.8,
                   edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5, label='Cognitive Engagement Index (CEI)')
        bars_complex = axes[0,1].bar(x_positions + bar_width, complex_engagement, 
                   width=bar_width, color=PURPLE_PALETTE['warm_gold'], alpha=0.8,
                   edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5, label='Composite Engagement Score')
        axes[0,1].set_title('Engagement Metrics per Fascia d\'Et√†', color=PURPLE_PALETTE['dark_purple'])
        axes[0,1].set_ylabel('Engagement Metrics')
        axes[0,1].set_xticks(x_positions)
        axes[0,1].set_xticklabels(age_groups, rotation=45, color=PURPLE_PALETTE['dark_purple'])
        axes[0,1].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        axes[0,1].legend(loc='upper left', fontsize=10, frameon=False)
        crosstab = pd.crosstab(self.df['eta_std'], self.df['socio_std'], margins=True)
        purple_cmap = sns.color_palette(PURPLE_SEQUENTIAL, as_cmap=True)
        sns.heatmap(crosstab.iloc[:-1, :-1], annot=True, fmt='d', cmap=purple_cmap, 
                   ax=axes[1,0], cbar_kws={'label': 'Numero Partecipanti'},
                   annot_kws={'color': PURPLE_PALETTE['dark_purple']})
        axes[1,0].set_title('Heatmap: Et√† √ó Status Socio', color=PURPLE_PALETTE['dark_purple'])
        axes[1,0].set_xlabel('Status Socio')
        axes[1,0].set_ylabel('Fascia d\'Et√†')
        conversion_potential = [d['conversion_potential'] for d in gap_data]
        bars4 = axes[1,1].bar(age_groups, conversion_potential, 
                             color=PURPLE_PALETTE['warm_gold'], alpha=0.8,
                             edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[1,1].set_title('Potenziale di Conversione per Fascia d\'Et√†', color=PURPLE_PALETTE['dark_purple'])
        axes[1,1].set_ylabel('Potenziale Conversione')
        axes[1,1].tick_params(axis='x', rotation=45, colors=PURPLE_PALETTE['dark_purple'])
        axes[1,1].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        plt.tight_layout()
        plt.savefig(self.output_dir / 'generational_membership_gap.png', facecolor='white', edgecolor='none')
        plt.close()
    
    def _plot_satisfaction_distribution(self):
        """Visualizza la distribuzione della soddisfazione."""
        fig, axes = plt.subplots(2,2, figsize=(16, 16))
        fig.suptitle('Analisi Distribuzione Soddisfazione', fontsize=16, fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        # 1. Distribuzione generale
        satisfaction_counts = self.df['soddisfazione_num'].value_counts().sort_index()
        colors = [PURPLE_PALETTE['primary_purple'] if score == 2 else PURPLE_PALETTE['warm_gold'] for score in satisfaction_counts.index]
        bars1 = axes[0,0].bar(satisfaction_counts.index, satisfaction_counts.values, 
                             color=colors, alpha=0.8,
                             edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[0,0].set_title('Distribuzione Generale della Soddisfazione', color=PURPLE_PALETTE['dark_purple'])
        axes[0,0].set_xlabel('Livello Soddisfazione')
        axes[0,0].set_ylabel('Numero Partecipanti')
        axes[0,0].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        total = sum(satisfaction_counts.values)
        for bar, count in zip(bars1, satisfaction_counts.values):
            percentage = (count / total) * 100
            axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                          f'{count}\n({percentage:.1f}%)', ha='center', va='bottom', 
                          fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        # 2. Soddisfazione per modalit√†
        satisfaction_by_mode = pd.crosstab(self.df['Fonte'], self.df['soddisfazione_num'])
        satisfaction_by_mode.plot(kind='bar', ax=axes[0,1], 
                                 color=[PURPLE_PALETTE['light_purple'], PURPLE_PALETTE['royal_purple']],
                                 alpha=0.8, edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[0,1].set_title('Soddisfazione per Modalit√† di Fruizione', color=PURPLE_PALETTE['dark_purple'])
        axes[0,1].set_xlabel('Modalit√†')
        axes[0,1].set_ylabel('Numero Partecipanti')
        axes[0,1].legend(['Soddisfatto (2)', 'Molto Soddisfatto (3)'])
        axes[0,1].tick_params(axis='x', rotation=45, colors=PURPLE_PALETTE['dark_purple'])
        axes[0,1].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        # 3. Soddisfazione per fascia d'et√†
        satisfaction_by_age = pd.crosstab(self.df['eta_std'], self.df['soddisfazione_num'])
        satisfaction_by_age.plot(kind='bar', ax=axes[1,0], 
                                color=[PURPLE_PALETTE['periwinkle'], PURPLE_PALETTE['sage']],
                                alpha=0.8, edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[1,0].set_title('Soddisfazione per Fascia d\'Et√†', color=PURPLE_PALETTE['dark_purple'])
        axes[1,0].set_xlabel('Fascia d\'Et√†')
        axes[1,0].set_ylabel('Numero Partecipanti')
        axes[1,0].legend(['Soddisfatto (2)', 'Molto Soddisfatto (3)'])
        axes[1,0].tick_params(axis='x', rotation=45, colors=PURPLE_PALETTE['dark_purple'])
        axes[1,0].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        # 4. Box plot comparativo
        satisfaction_data = [
            self.df[self.df['Fonte'] == 'Cartaceo']['soddisfazione_num'],
            self.df[self.df['Fonte'] == 'Webinar']['soddisfazione_num']
        ]
        box_plot = axes[1,1].boxplot(satisfaction_data, labels=['Cartaceo', 'Webinar'],
                                    patch_artist=True, notch=True)
        colors_box = [PURPLE_PALETTE['dusty_rose'], PURPLE_PALETTE['periwinkle']]
        for patch, color in zip(box_plot['boxes'], colors_box):
            patch.set_facecolor(color)
            patch.set_alpha(0.8)
            patch.set_edgecolor(PURPLE_PALETTE['dark_purple'])
            patch.set_linewidth(1.5)
        axes[1,1].set_title('Distribuzione Soddisfazione: Cartaceo vs Webinar', color=PURPLE_PALETTE['dark_purple'])
        axes[1,1].set_ylabel('Livello Soddisfazione')
        axes[1,1].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        plt.tight_layout()
        plt.savefig(self.output_dir / 'satisfaction_distribution.png', facecolor='white', edgecolor='none')
        plt.close()
    
    def _plot_satisfaction_elements(self):
        """Visualizza la distribuzione degli elementi di soddisfazione."""
        satisfaction_elements = []
        potential_columns = ['Dibattito', 'comunicative', 'Argomento']
        for col in potential_columns:
            if col in self.df.columns:
                satisfaction_elements.append(col)
        if not satisfaction_elements:
            return
        n_elements = len(satisfaction_elements)
        n_cols = min(3, n_elements)
        n_rows = (n_elements + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))
        if n_elements == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = axes.reshape(1, -1)
        fig.suptitle('Distribuzione Elementi di Soddisfazione', fontsize=16, fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        for i, element in enumerate(satisfaction_elements):
            row = i // n_cols
            col = i % n_cols
            ax = axes[row, col] if n_rows > 1 else axes[col]
            element_counts = self.df[element].value_counts().sort_index()
            colors_gradient = [PURPLE_CATEGORICAL[j % len(PURPLE_CATEGORICAL)] for j in range(len(element_counts))]
            bars = ax.bar(element_counts.index, element_counts.values, 
                         color=colors_gradient, alpha=0.8,
                         edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
            ax.set_title(element, color=PURPLE_PALETTE['dark_purple'])
            ax.set_ylabel('Numero Partecipanti')
            ax.grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
            for bar, count in zip(bars, element_counts.values):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                       f'{count}', ha='center', va='bottom', fontweight='bold',
                       color=PURPLE_PALETTE['dark_purple'])
        total_plots = n_rows * n_cols
        for i in range(n_elements, total_plots):
            row = i // n_cols
            col = i % n_cols
            if n_rows > 1:
                axes[row, col].set_visible(False)
            else:
                axes[col].set_visible(False)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'satisfaction_elements.png', facecolor='white', edgecolor='none')
        plt.close()
    
    def _plot_communication_channels(self):
        """Analizza l'efficacia dei canali di comunicazione"""
        comm_column = 'Conoscenza' if 'Conoscenza' in self.df.columns else 'canali_clean'
        if comm_column not in self.df.columns:
            return
        
        all_channels = []
        for channels_raw in self.df[comm_column].dropna():
            if channels_raw != 'N/A' and pd.notna(channels_raw):
                channels_str = str(channels_raw).strip()
                if ',' in channels_str:
                    separators = [',']
                elif ';' in channels_str:
                    separators = [';']
                else:
                    separators = [None]
                if separators[0] is None:
                    channel_clean = channels_str.strip()
                    if channel_clean and channel_clean != 'N/A':
                        all_channels.append(channel_clean)
                else:
                    for sep in separators:
                        channel_list = [c.strip() for c in channels_str.split(sep)]
                        channel_list = [c for c in channel_list 
                                      if c and c != 'N/A' and len(c.strip()) > 2]
                        all_channels.extend(channel_list)
                        break 
        if not all_channels:
            return
        normalized_channels = []
        for channel in all_channels:
            channel_lower = channel.lower().strip()
            if 'newsletter' in channel_lower or 'news letter' in channel_lower:
                normalized_channels.append('Newsletter dell\'Accademia')
            elif 'passaparola' in channel_lower or 'passa parola' in channel_lower:
                normalized_channels.append('Passaparola')
            elif 'invito' in channel_lower and 'socio' in channel_lower:
                normalized_channels.append('Invito di un/una socio/a')
            elif 'sito' in channel_lower and 'web' in channel_lower:
                normalized_channels.append('Sito web (www.agiati.org)')
            elif 'manifesti' in channel_lower or 'locandine' in channel_lower:
                normalized_channels.append('Manifesti o locandine')
            elif 'quotidiani' in channel_lower or 'giornali' in channel_lower:
                normalized_channels.append('Quotidiani locali')
            elif 'social' in channel_lower or 'facebook' in channel_lower or 'instagram' in channel_lower:
                normalized_channels.append('Siti e canali social di altri enti')
            else:
                # mantieni il nome originale se non trova corrispondenze
                normalized_channels.append(channel.strip())
        channel_counts = Counter(normalized_channels)
        channel_counts = {k: v for k, v in channel_counts.items() if v >= 1}
        if not channel_counts:
            return
        sorted_channels = sorted(channel_counts.items(), key=lambda x: x[1], reverse=True)
        top_channels = sorted_channels[:10]
        channels = [item[0] for item in top_channels]
        counts = [item[1] for item in top_channels]
        fig, ax = plt.subplots(figsize=(14, max(8, len(channels) * 0.8)))
        colors = [PURPLE_SEQUENTIAL[min(i, len(PURPLE_SEQUENTIAL)-1)] for i in range(len(channels))]
        bars = ax.barh(range(len(channels)), counts, color=colors, alpha=0.8,
                      edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        ax.set_yticks(range(len(channels)))
        ax.set_yticklabels(channels, fontsize=11, color=PURPLE_PALETTE['dark_purple'])
        ax.set_xlabel('Numero di Menzioni', fontsize=12, fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        ax.set_title('Efficacia Canali di Comunicazione\n(Analisi Scelte Multiple)', 
                    fontsize=14, fontweight='bold', pad=20, color=PURPLE_PALETTE['dark_purple'])
        ax.grid(axis='x', alpha=0.3, linestyle='--', color=PURPLE_PALETTE['light_purple'])
        total_mentions = sum(counts)
        for i, (bar, count) in enumerate(zip(bars, counts)):
            percentage = (count / total_mentions) * 100
            ax.text(bar.get_width() + max(counts) * 0.01, 
                   bar.get_y() + bar.get_height()/2, 
                   f'{count} ({percentage:.1f}%)', 
                   va='center', ha='left', fontweight='bold', fontsize=10,
                   color=PURPLE_PALETTE['dark_purple'])
        stats_text = f'Totale menzioni: {total_mentions}\nCanali unici: {len(channel_counts)}\nRisposte multiple: {len(all_channels) - len(self.df[comm_column].dropna())}'
        ax.text(0.98, 0.02, stats_text, transform=ax.transAxes, 
               bbox=dict(boxstyle='round', facecolor=PURPLE_PALETTE['lavender'], alpha=0.9,
                        edgecolor=PURPLE_PALETTE['primary_purple']),
               va='bottom', ha='right', fontsize=9, color=PURPLE_PALETTE['dark_purple'])
        plt.tight_layout()
        plt.savefig(self.output_dir / 'communication_channels.png', 
                   dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
        plt.close()
        print(f"\tAnalisi canali completata: {len(channel_counts)} canali unici, {total_mentions} menzioni totali")

    def _plot_ces_analysis(self):
        """Visualizza l'analisi del Cultural Engagement Score con tema purple."""
        ces_data = self.results['ces']
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Analisi del Cultural Engagement Score (CES)', fontsize=16, fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        # 1. Componenti del CES
        components = ces_data['components']
        comp_names = ['Satisfaction\nFoundation', 'Membership\nAmplification', 
                     'Cognitive\nEngagement']
        comp_values = [components['satisfaction'], components['membership'], 
                      components['cognitive_engagement_mean']]
        colors_comp = [PURPLE_PALETTE['warm_gold'], PURPLE_PALETTE['royal_purple'], PURPLE_PALETTE['sage']]
        bars1 = axes[0,0].bar(comp_names, comp_values, 
                             color=colors_comp, alpha=0.8,
                             edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        axes[0,0].set_title('Componenti del Cultural Engagement Score', color=PURPLE_PALETTE['dark_purple'])
        axes[0,0].set_ylabel('Valore Componente')
        axes[0,0].set_ylim(0, 1.0)
        axes[0,0].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        for bar, value in zip(bars1, comp_values):
            axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                          f'{value:.3f}', ha='center', va='bottom', fontweight='bold',
                          color=PURPLE_PALETTE['dark_purple'])
        # 2. CES Score finale (Gauge plot migliorato)
        current_ces = ces_data['score']
        max_ces = ces_data['components']['max_theoretical']
        theta = np.linspace(0, np.pi, 100)
        r = 1
        axes[0,1].plot(r * np.cos(theta), r * np.sin(theta), color=PURPLE_PALETTE['dark_purple'], linewidth=3)
        axes[0,1].fill_between(r * np.cos(theta), 0, r * np.sin(theta), alpha=0.2, color=PURPLE_PALETTE['lavender'])
        current_angle = (1 - current_ces / max_ces) * np.pi
        axes[0,1].plot([0, r * np.cos(current_angle)], [0, r * np.sin(current_angle)], 
                      color=PURPLE_PALETTE['primary_purple'], linewidth=6, 
                      label=f'CES Attuale: {current_ces:.3f}')
        axes[0,1].scatter(r * np.cos(current_angle), r * np.sin(current_angle), 
                         color=PURPLE_PALETTE['warm_gold'], s=100, zorder=5, edgecolor=PURPLE_PALETTE['dark_purple'])
        axes[0,1].set_xlim(-1.2, 1.2)
        axes[0,1].set_ylim(-0.2, 1.2)
        axes[0,1].set_aspect('equal')
        axes[0,1].set_title(f'CES Score: {current_ces:.3f}/{max_ces:.1f}', color=PURPLE_PALETTE['dark_purple'])
        axes[0,1].text(0, -0.1, f'Percentile: {ces_data["percentile_rank"]:.1f}%', 
                      ha='center', fontsize=12, fontweight='bold', color=PURPLE_PALETTE['dark_purple'])
        axes[0,1].axis('off')
        # 3. Scenari di miglioramento
        scenarios = ces_data['improvement_scenarios']
        scenario_names = list(scenarios.keys())
        projected_ces = [scenarios[s]['projected_ces'] for s in scenario_names]
        improvements = [scenarios[s]['improvement_percentage'] for s in scenario_names]
        colors_scenarios = [PURPLE_PALETTE['periwinkle'], PURPLE_PALETTE['warm_gold'], PURPLE_PALETTE['dusty_rose']]
        bars3 = axes[1,0].bar(scenario_names, projected_ces, alpha=0.8,
                             color=colors_scenarios, edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1.5)
        # Linea baseline
        axes[1,0].axhline(y=current_ces, color=PURPLE_PALETTE['dark_purple'], linestyle='--', 
                         linewidth=2, label=f'Baseline: {current_ces:.3f}')
        axes[1,0].set_title('Scenari di Miglioramento CES', color=PURPLE_PALETTE['dark_purple'])
        axes[1,0].set_ylabel('CES Proiettato')
        axes[1,0].legend()
        axes[1,0].tick_params(axis='x', rotation=45, colors=PURPLE_PALETTE['dark_purple'])
        axes[1,0].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        for bar, improvement in zip(bars3, improvements):
            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                          f'+{improvement:.1f}%', ha='center', va='bottom', 
                          fontweight='bold', color=PURPLE_PALETTE['sage'])
        
        # 4. Distribuzione Bootstrap
        bootstrap_data = ces_data['bootstrap_validation']
        bootstrap_scores = np.random.normal(bootstrap_data['mean'], 
                                          bootstrap_data['std_error'], 1000)
        axes[1,1].hist(bootstrap_scores, bins=30, alpha=0.7, color=PURPLE_PALETTE['periwinkle'], 
                      density=True, label='Bootstrap Distribution',
                      edgecolor=PURPLE_PALETTE['dark_purple'], linewidth=1)
        axes[1,1].axvline(bootstrap_data['ci_95_lower'], color=PURPLE_PALETTE['dusty_rose'], 
                         linestyle='--', linewidth=2, label='CI 95%')
        axes[1,1].axvline(bootstrap_data['ci_95_upper'], color=PURPLE_PALETTE['dusty_rose'], 
                         linestyle='--', linewidth=2)
        axes[1,1].axvline(bootstrap_data['mean'], color=PURPLE_PALETTE['dark_purple'], 
                         linestyle='-', linewidth=2, label='Media Bootstrap')
        axes[1,1].axvline(current_ces, color=PURPLE_PALETTE['warm_gold'], 
                         linestyle='-', linewidth=3, label='CES Corrente')
        axes[1,1].set_title('Validazione Bootstrap CES', color=PURPLE_PALETTE['dark_purple'])
        axes[1,1].set_xlabel('CES Score')
        axes[1,1].set_ylabel('Densit√†')
        axes[1,1].legend()
        axes[1,1].grid(alpha=0.3, color=PURPLE_PALETTE['light_purple'])
        plt.tight_layout()
        plt.savefig(self.output_dir / 'ces_analysis.png', facecolor='white', edgecolor='none')
        plt.close()

    def generate_comprehensive_report(self) -> str:
        """
        Genera un report testuale completo con tutte le statistiche e analisi.
        Returns:
        --------
        str
            Report completo formattato
        """
        print("Generazione report completo...")
        report = []
        report.append("="*100)
        report.append("ANALISI QUANTITATIVA DEI DATI DI SODDISFAZIONE")
        report.append("Accademia Roveretana degli Agiati")
        report.append("="*100)
        report.append(f"Data di generazione: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        report.append(f"Versione script: 2.1 (Correzione Metodologica Critica + Grafici Tesi)")
        report.append(f"Autore: Giuseppe Pio Mangiacotti")
        report.append(f"Istituzione: Universit√† degli Studi di Trento")
        report.append("")

        report.append("SOMMARIO ESECUTIVO")
        report.append("-"*50)
        report.append(f"‚Ä¢ Dataset: {len(self.df)} record completi analizzati")
        report.append(f"‚Ä¢ Metodologia: Fisher Exact Test (principale), Regressione Logistica (validazione)")
        report.append(f"‚Ä¢ Key Findings: 3 pattern principali identificati")
        
        if 'ces' in self.results:
            ces_score = self.results['ces']['score']
            ces_percentile = self.results['ces']['percentile_rank']
            report.append(f"‚Ä¢ Cultural Engagement Score: {ces_score:.3f} ({ces_percentile:.1f}¬∞ percentile)")
        report.append("")
        
        # === CORREZIONI METODOLOGICHE ===
        report.append(" CORREZIONI METODOLOGICHE VERSIONE 2.1")
        report.append("-"*50)
        report.append("PROBLEMA RISOLTO:")
        report.append("‚Ä¢ Test Mann-Whitney U inappropriato per dati con ties eccessivi (100% ties)")
        report.append("‚Ä¢ Solo valori 2 (Soddisfatto) e 3 (Molto Soddisfatto) nel dataset")
        report.append("‚Ä¢ Perdita significativa di potenza statistica")
        report.append("")
        report.append("SOLUZIONE IMPLEMENTATA:")
        report.append("‚Ä¢ Fisher Exact Test per confronto proporzioni binarie")
        report.append("‚Ä¢ Tabelle di contingenza 2√ó2 appropriate")
        report.append("‚Ä¢ Odds ratio e p-value esatti")
        report.append("‚Ä¢ Regressione logistica per validazione terziaria completa")
        report.append("‚Ä¢ Warning automatici per test inappropriati")
        report.append("")
        # === PANORAMICA DEMOGRAFICA ===
        report.append("PANORAMICA DEMOGRAFICA")
        report.append("-"*50)
        # Distribuzione et√†
        age_dist = self.df['eta_std'].value_counts().sort_index()
        report.append("Distribuzione per Fascia d'Et√†:")
        for age, count in age_dist.items():
            percentage = (count / len(self.df)) * 100
            report.append(f"  ‚Ä¢ {age}: {count} partecipanti ({percentage:.1f}%)")
        # Status soci
        member_dist = self.df['socio_std'].value_counts()
        report.append("\nStatus di Socio:")
        for status, count in member_dist.items():
            percentage = (count / len(self.df)) * 100
            report.append(f"  ‚Ä¢ {status}: {count} partecipanti ({percentage:.1f}%)")
        
        # Modalit√† di fruizione
        mode_dist = self.df['Fonte'].value_counts()
        report.append("\nModalit√† di Fruizione:")
        for mode, count in mode_dist.items():
            percentage = (count / len(self.df)) * 100
            report.append(f"  ‚Ä¢ {mode}: {count} partecipanti ({percentage:.1f}%)")
        
        # Soddisfazione generale
        satisfaction_dist = self.df['soddisfazione_num'].value_counts().sort_index()
        avg_satisfaction = self.df['soddisfazione_num'].mean()
        report.append(f"\nSoddisfazione Generale:")
        report.append(f"  ‚Ä¢ Media: {avg_satisfaction:.3f}")
        for score, count in satisfaction_dist.items():
            percentage = (count / len(self.df)) * 100
            label = "Soddisfatto" if score == 2 else "Molto Soddisfatto"
            report.append(f"  ‚Ä¢ {label} ({score}): {count} partecipanti ({percentage:.1f}%)")
        report.append("")
        
        # === KEY FINDINGS ===
        if 'key_findings' in self.results:
            report.append("üéØ KEY FINDINGS PRINCIPALI")
            report.append("-"*50)
            
            # 1. Digital Satisfaction Paradox
            if 'digital_paradox' in self.results['key_findings']:
                report.append("1. DIGITAL SATISFACTION PARADOX")
                report.append("   " + "="*40)
                paradox_data = self.results['key_findings']['digital_paradox']
                
                report.append("   üìä STATISTICHE DESCRITTIVE:")
                report.append(f"   ‚Ä¢ Cartaceo: N={paradox_data['cartaceo_stats']['n']}, Media={paradox_data['cartaceo_stats']['mean']:.3f}")
                report.append(f"   ‚Ä¢ Webinar: N={paradox_data['webinar_stats']['n']}, Media={paradox_data['webinar_stats']['mean']:.3f}")
                report.append(f"   ‚Ä¢ Differenza: +{paradox_data['difference']:.3f} punti (Webinar > Cartaceo)")
                
                report.append("   \n   ü•á FISHER EXACT TEST (Metodo Principale):")
                report.append(f"   ‚Ä¢ Odds Ratio: {paradox_data['fisher_odds_ratio']:.3f}")
                report.append(f"   ‚Ä¢ p-value: {paradox_data['fisher_p_value']:.4f}")
                significance = "***" if paradox_data['fisher_p_value'] < 0.001 else "**" if paradox_data['fisher_p_value'] < 0.01 else "*" if paradox_data['fisher_p_value'] < 0.05 else "n.s."
                report.append(f"   ‚Ä¢ Significativit√†: {significance}")
                report.append(f"   ‚Ä¢ Differenza Proporzioni: +{paradox_data['proportion_difference']:.3f}")
                
                # Tabella di contingenza
                contingency = paradox_data['contingency_table']
                report.append("   \n   üìã Tabella di Contingenza 2√ó2:")
                report.append("                      Soddisfatto(2)  Molto Sodd.(3)")
                report.append(f"     Cartaceo:        {contingency[0][0]:>11}  {contingency[0][1]:>13}")
                report.append(f"     Webinar:         {contingency[1][0]:>11}  {contingency[1][1]:>13}")
                
                # Regressione logistica
                if paradox_data['logistic_regression'].get('model_available', False):
                    lr_data = paradox_data['logistic_regression']
                    report.append("   \n   üî¨ REGRESSIONE LOGISTICA (Validazione):")
                    report.append(f"   ‚Ä¢ Œ≤ (Webinar): {lr_data['beta']:.3f} ¬± {lr_data['se_beta']:.3f}")
                    report.append(f"   ‚Ä¢ Odds Ratio: {lr_data['odds_ratio']:.3f}")
                    report.append(f"   ‚Ä¢ CI 95% (OR): [{lr_data['or_ci_95'][0]:.3f}, {lr_data['or_ci_95'][1]:.3f}]")
                    report.append(f"   ‚Ä¢ p-value: {lr_data['p_value']:.4f}")
                    report.append(f"   ‚Ä¢ Pseudo R¬≤: {lr_data['pseudo_r2_mcfadden']:.3f}")
                    report.append(f"   ‚Ä¢ Equazione: {lr_data['model_equation']}")
                    
                    # Convergenza metodologica
                    or_diff = abs(paradox_data['fisher_odds_ratio'] - lr_data['odds_ratio'])
                    convergence_status = "ECCELLENTE" if or_diff < 0.1 else "ACCETTABILE" if or_diff < 0.5 else "PROBLEMATICA"
                    report.append(f"   ‚Ä¢ Convergenza metodologica: {convergence_status} (diff OR: {or_diff:.3f})")
                
                # Mann-Whitney con disclaimer
                report.append("   \n   ‚ö†Ô∏è  MANN-WHITNEY U (Limitazioni):")
                report.append(f"   ‚Ä¢ U-statistic: {paradox_data['mannwhitney_u']:.1f}")
                report.append(f"   ‚Ä¢ p-value: {paradox_data['mannwhitney_p']:.4f}")
                report.append(f"   ‚Ä¢ Ties: {paradox_data['ties_percentage']:.1%} (ECCESSIVI)")
                report.append(f"   ‚Ä¢ Limitazione: Solo 2 valori distinti, violazione assunzioni")
                
                # Age Paradox
                report.append("   \n   üîç AGE PARADOX:")
                report.append(f"   ‚Ä¢ Et√† media Webinar: {paradox_data['age_webinar']:.1f} anni")
                report.append(f"   ‚Ä¢ Et√† media Cartaceo: {paradox_data['age_cartaceo']:.1f} anni")
                report.append(f"   ‚Ä¢ Differenza: +{paradox_data['age_paradox']:.1f} anni (controintuitiva)")
                report.append("")
            
            # 2. Generational Membership Gap
            if 'generational_gap' in self.results['key_findings']:
                report.append("2. GENERATIONAL MEMBERSHIP GAP")
                report.append("   " + "="*40)
                gap_data = self.results['key_findings']['generational_gap']
                
                report.append("   üìà ENGAGEMENT METRICS PER FASCIA D'ET√Ä:")
                for metric in gap_data['engagement_metrics']:
                    report.append(f"   ‚Ä¢ {metric['age_group']}:")
                    report.append(f"     - Dimensione coorte: {metric['cohort_size']}")
                    report.append(f"     - Tasso membership: {metric['membership_rate']:.1%}")
                    report.append(f"     - Soddisfazione media: {metric['avg_satisfaction']:.3f}")
                    report.append(f"     - Institutional Engagement Index: {metric['engagement_index']:.3f}")
                    report.append(f"     - Cognitive Engagement Index: {metric['cognitive_engagement_mean']:.3f}")
                    report.append(f"     - Composite Engagement Index: {(metric['cognitive_engagement_mean'] + metric['engagement_index']) / 2 :.3f}")
                    report.append(f"     - Potenziale conversione: {metric['conversion_potential']:.1f}")
                
                report.append("   \n   üìä CORRELAZIONE ET√Ä-MEMBERSHIP:")
                report.append(f"   ‚Ä¢ Correlazione Spearman (principale): r = {gap_data['spearman_correlation']:.3f}")
                report.append(f"   ‚Ä¢ p-value: {gap_data['spearman_p_value']:.4f}")
                report.append(f"   ‚Ä¢ Correlazione Pearson (confronto): r = {gap_data['pearson_correlation']:.3f}")
                report.append(f"   ‚Ä¢ Dimensione campione: {gap_data['sample_size']} coorti")
                report.append(f"   ‚Ä¢ Metodo raccomandato: {gap_data['recommended_method']}")
                report.append("")
            
            # 3. Inverse Contentment Effect
            if 'contentment_effect' in self.results['key_findings']:
                report.append("3. INVERSE CONTENTMENT EFFECT")
                report.append("   " + "="*40)
                content_data = self.results['key_findings']['contentment_effect']
                
                report.append("   üìù METRICHE FEEDBACK:")
                satisfied_metrics = content_data['feedback_metrics']['satisfied']
                very_satisfied_metrics = content_data['feedback_metrics']['very_satisfied']
                
                report.append(f"   ‚Ä¢ Soddisfatti (2):")
                report.append(f"     - Totale: {satisfied_metrics['total']}")
                report.append(f"     - Con feedback: {satisfied_metrics['with_feedback']}")
                report.append(f"     - Tasso feedback: {satisfied_metrics['feedback_rate']:.1%}")
                
                report.append(f"   ‚Ä¢ Molto Soddisfatti (3):")
                report.append(f"     - Totale: {very_satisfied_metrics['total']}")
                report.append(f"     - Con feedback: {very_satisfied_metrics['with_feedback']}")
                report.append(f"     - Tasso feedback: {very_satisfied_metrics['feedback_rate']:.1%}")
                
                report.append(f"   \n   üìä CONTENTMENT RATIO: {content_data['contentment_ratio']:.3f}")
                report.append(f"   ‚Ä¢ Effetto confermato: {'S√¨' if content_data['effect_confirmed'] else 'No'}")
                
                report.append(f"   \n   üß™ TEST STATISTICO ({content_data['primary_test']}):")
                report.append(f"   ‚Ä¢ p-value: {content_data['primary_p_value']:.4f}")
                
                if content_data['primary_test'] == 'Chi-quadro':
                    report.append(f"   ‚Ä¢ Chi¬≤ statistic: {content_data['chi2_statistic']:.3f}")
                    cochran = content_data['cochran_criteria']
                    report.append(f"   ‚Ä¢ Criteri Cochran: {'Soddisfatti' if cochran['satisfied'] else 'Violati'}")
                    report.append(f"   ‚Ä¢ Min freq. attesa: {cochran['min_expected_frequency']:.1f}")
                else:  # Fisher Exact Test
                    report.append(f"   ‚Ä¢ Fisher Odds Ratio: {content_data['fisher_odds_ratio']:.3f}")
                    cochran = content_data['cochran_criteria']
                    report.append(f"   ‚Ä¢ Motivo Fisher: Min freq. attesa = {cochran['min_expected_frequency']:.1f}")
                
                report.append("")
        
        # === CULTURAL ENGAGEMENT SCORE ===
        if 'ces' in self.results:
            report.append("CULTURAL ENGAGEMENT SCORE (CES)")
            report.append("-"*50)
            ces_data = self.results['ces']
            
            report.append("SCORE E COMPONENTI:")
            report.append(f"‚Ä¢ CES Score: {ces_data['score']:.3f}")
            report.append(f"‚Ä¢ Percentile Rank: {ces_data['percentile_rank']:.1f}%")
            report.append(f"‚Ä¢ Max Teorico: {ces_data['components']['max_theoretical']:.1f}")
            
            report.append("\nCOMPONENTI:")
            components = ces_data['components']
            report.append(f"‚Ä¢ Satisfaction: {components['satisfaction']:.3f}")
            report.append(f"‚Ä¢ Membership Rate: {components['membership']:.3f}")
            report.append(f"‚Ä¢ Cognitive Engagement: {components['cognitive_engagement_mean']:.3f}")

            # Bootstrap validation
            bootstrap = ces_data['bootstrap_validation']
            report.append(f"\nVALIDAZIONE BOOTSTRAP:")
            report.append(f"‚Ä¢ Media Bootstrap: {bootstrap['mean']:.3f}")
            report.append(f"‚Ä¢ Errore Standard: {bootstrap['std_error']:.3f}")
            report.append(f"‚Ä¢ CI 95%: [{bootstrap['ci_95_lower']:.3f}, {bootstrap['ci_95_upper']:.3f}]")
            report.append(f"‚Ä¢ Coefficiente Variazione: {bootstrap['cv']:.1%}")
            
            # Scenari di miglioramento
            report.append(f"\nSCENARI DI MIGLIORAMENTO:")
            scenarios = ces_data['improvement_scenarios']
            for scenario_name, scenario_data in scenarios.items():
                report.append(f"‚Ä¢ {scenario_name.title()}:")
                report.append(f"  - CES proiettato: {scenario_data['projected_ces']:.3f}")
                report.append(f"  - Miglioramento: +{scenario_data['improvement_percentage']:.1f}%")
                report.append(f"  - Target membership: {scenario_data['target_membership']:.1%}")
                report.append(f"  - Target engagement cognitivo: {scenario_data['target_cognitive']:.1%}")
            report.append("")
        
        # === QUALIT√Ä METODOLOGICA ===
        if self.test_appropriateness:
            report.append("üî¨ ASSESSMENT QUALIT√Ä METODOLOGICA")
            report.append("-"*50)
            report.append("TEST STATISTICI UTILIZZATI:")
            
            for test_name, assessment in self.test_appropriateness.items():
                status_icon = "‚úÖ" if assessment['is_appropriate'] else "‚ùå"
                report.append(f"{status_icon} {test_name}:")
                report.append(f"  ‚Ä¢ Dati: {assessment['data_description']}")
                report.append(f"  ‚Ä¢ Appropriatezza: {assessment['reason']}")
                report.append(f"  ‚Ä¢ Raccomandazione: {assessment['recommendation']}")
                report.append("")
        
        # === AVVISI METODOLOGICI ===
        if self.methodological_warnings:
            report.append("‚ö†Ô∏è  AVVISI METODOLOGICI")
            report.append("-"*50)
            for warning in self.methodological_warnings:
                report.append(f"‚Ä¢ {warning['test']}: {warning['warning']}")
            report.append("")
        
        # === RACCOMANDAZIONI ===
        report.append("üí° RACCOMANDAZIONI METODOLOGICHE")
        report.append("-"*50)
        report.append("‚úÖ UTILIZZARE:")
        report.append("‚Ä¢ Fisher Exact Test come risultato principale per confronti binari")
        report.append("‚Ä¢ Regressione Logistica per validazione e intervalli di confidenza")
        report.append("‚Ä¢ Bootstrap resampling per validazione indicatori compositi")
        report.append("‚Ä¢ Correlazione di Spearman per campioni piccoli")
        report.append("")
        report.append("‚ö†Ô∏è  EVITARE:")
        report.append("‚Ä¢ Mann-Whitney U per dati con ties eccessivi (>50%)")
        report.append("‚Ä¢ Test œá¬≤ quando violati i criteri di Cochran")
        report.append("‚Ä¢ Correlazione di Pearson senza verifica normalit√†")
        report.append("")
        report.append("üìä RIPORTARE:")
        report.append("‚Ä¢ Odds Ratio con intervalli di confidenza al 95%")
        report.append("‚Ä¢ Tabelle di contingenza complete per trasparenza")
        report.append("‚Ä¢ Dimensioni campionarie per tutti i sottogruppi")
        report.append("‚Ä¢ Assessment appropriatezza dei test utilizzati")
        report.append("")
        
        # === LIMITAZIONI E FUTURE DIREZIONI ===
        report.append("üöß LIMITAZIONI DELLO STUDIO")
        report.append("-"*50)
        report.append("‚Ä¢ Campione di convenienza limitato a partecipanti eventi specifici")
        report.append("‚Ä¢ Scala di soddisfazione ridotta (solo valori 2-3 osservati)")
        report.append("‚Ä¢ Analisi cross-sectional, non longitudinale")
        report.append("‚Ä¢ Possibili bias di selezione tra modalit√† cartacea e digitale")
        report.append("‚Ä¢ Dimensioni campionarie ridotte per alcune sottopopolazioni")
        report.append("")
        
        report.append("üîÆ FUTURE DIREZIONI DI RICERCA")
        report.append("-"*50)
        report.append("‚Ä¢ Espansione scala di misurazione soddisfazione")
        report.append("‚Ä¢ Studio longitudinale per tracking engagement nel tempo")
        report.append("‚Ä¢ Campionamento randomizzato tra modalit√† di fruizione")
        report.append("‚Ä¢ Analisi qualitativa approfondita del feedback testuale")
        report.append("‚Ä¢ Validazione CES su istituzioni culturali comparabili")
        report.append("")
        
        # === CITAZIONE ===
        report.append("üìö CITAZIONE")
        report.append("-"*50)
        report.append("Mangiacotti, G.P. (2025). Progettazione di interfacce per la raccolta")
        report.append("di feedback nel settore culturale: un sistema ibrido per l'Accademia")
        report.append("Roveretana degli Agiati v2.1. Universit√† degli Studi di Trento.")
        report.append("")
        
        # === APPENDICE TECNICA ===
        report.append("üîß APPENDICE TECNICA")
        report.append("-"*50)
        report.append(f"‚Ä¢ Versione Python: 3.9.0")
        report.append(f"‚Ä¢ Librerie principali: pandas, scipy, sklearn, matplotlib, seaborn")
        report.append(f"‚Ä¢ Metodo bootstrap: {1000} iterazioni con rimpiazzamento")
        report.append(f"‚Ä¢ Test di significativit√†: Œ± = 0.05")
        report.append(f"‚Ä¢ Intervalli di confidenza: 95%")
        report.append(f"‚Ä¢ Correzioni multiple: Non applicate (analisi esploratoria)")
        report.append("")
        
        report.append("="*100)
        report.append(f"Fine Report - {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        report.append("="*100)
        
        # Salva il report
        report_text = "\n".join(report)
        with open(self.output_dir / 'analysis_report_corrected.txt', 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        # Salva anche avvisi metodologici separatamente
        if self.methodological_warnings:
            warnings_text = "AVVISI METODOLOGICI - Accademia Agiati v2.1\n"
            warnings_text += "="*60 + "\n\n"
            for warning in self.methodological_warnings:
                warnings_text += f"TEST: {warning['test']}\n"
                warnings_text += f"PROBLEMA: {warning['warning']}\n"
                warnings_text += f"TIMESTAMP: {warning['timestamp']}\n"
                warnings_text += "-"*40 + "\n"
            
            with open(self.output_dir / 'methodological_warnings.txt', 'w', encoding='utf-8') as f:
                f.write(warnings_text)
        
        print(f"   ‚úì Report salvato in: {self.output_dir / 'analysis_report_corrected.txt'}")
        return report_text
    
    def run_complete_analysis(self):
        """
        Esegue l'analisi completa con tutti i componenti.
        """
        print("Avvio analisi completa Accademia Roveretana degli Agiati v2.1")
        print("="*80)
        self.load_and_clean_data()
        self.calculate_key_findings()
        self.calculate_cultural_engagement_score()
        self.print_detailed_digital_paradox_analysis()
        self.create_visualizations()
        self.generate_comprehensive_report()
        print("\n" + "="*80)
        print("ANALISI COMPLETATA CON SUCCESSO!")
        print("="*80)
        print(f"Output directory: {self.output_dir}")
        print("File generati:")
        print("\t-analysis_report_corrected.txt - Report completo")
        print("\t-methodological_warnings.txt - Avvisi metodologici")
        print("\t-contingency_table_fisher_test.csv - Tabella contingenza")
        print("\t-digital_satisfaction_paradox_corrected.png - Paradosso digitale")
        print("\t-demographic_overview_panel.png - üéì Pannello demografico (TESI)")
        print("\t-participation_modalities_panel.png - üéì Pannello partecipazione (TESI)")
        print("\t-+ altri grafici di analisi (ho perso il conto)")
        print("\nKey Findings identificati:")
        print("1. Digital Satisfaction Paradox")
        print("2. Generational Membership Gap")
        print("3. Inverse Contentment Effect")
        if 'ces' in self.results:
            ces_score = self.results['ces']['score']
            ces_percentile = self.results['ces']['percentile_rank']
            print(f"\nCultural Engagement Score: {ces_score:.3f} ({ces_percentile:.1f}¬∞ percentile)")
        print("\nMetodologia corretta applicata:")
        print("Fisher Exact Test (principale)")
        print("Regressione Logistica (validazione)")
        print("Bootstrap resampling (robustezza)")
        print("Mann-Whitney U (limitazioni dichiarate)")

def main():
    """Funzione principale per esecuzione da command line."""
    parser = argparse.ArgumentParser(description='Analisi dati Accademia Agiati v2.1')
    parser.add_argument('--input', required=True, help='Path al file CSV dei dati')
    parser.add_argument('--output', default='./output/', help='Directory di output')
    args = parser.parse_args()
    if not os.path.exists(args.input):
        print(f"File input non trovato: {args.input}")
        return
    analyzer = AccademiaAnalyzer(args.input, args.output)
    analyzer.run_complete_analysis()

if __name__ == "__main__":
    main()